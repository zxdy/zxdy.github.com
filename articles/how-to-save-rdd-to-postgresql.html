<!doctype html>
<html>
    <head>
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
        <meta name="description" content="Content of description meta tag"/>
        <meta name="keywords" content="代码,数据库,信息安全"/>
        <meta name="author" content="Content of author meta tag"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
        <title>离世庭院 - 如何保存spark结果到postgreSQL
</title>
        <link rel="stylesheet" href="/assets/vendor/normalize.css"/>
        <link rel="stylesheet" href="/assets/vendor/prettify-night.css"/>
        <link rel="stylesheet" href="/assets/themes/default/main.css"/>
        <link rel="shortcut icon" href="/fav.ico"/>
        <script type="text/javascript" src="/assets/vendor/prettify.js"></script>
        
    </head>
    <body onload="prettyPrint()">
    <div id="header">
        <div id="header-inner">
            <div id="title"><a href="/">离世庭院</a></div>
            <div id="subtitle"></div>
        </div>
    </div>

<div id="main">
    <div id="main-inner">
        <div id="topnav">
            <ul>
                <li><a href="/">首页</a></li>
                <li class="sep"> | </li>
                <li><a href="/tag.html">标签</a></li>
                
<li class="sep"> | </li>
<li><a href="/pages/about-me.html" target="_self">关于我</a></li>

<li class="sep"> | </li>
<li><a href="https://github.com/zxdy" target="_blank">github</a></li>


            </ul>
            <div style="clear:both;"></div>
        </div>
        <div id="article-title">
            <a href="/articles/how-to-save-rdd-to-postgresql.html">如何保存spark结果到postgreSQL</a>
        </div>
        <div id="article-meta">
            作者 Ario | 发布于 2015-05-06
        </div>
        <div id="article-tags">
        
        <a class="tag" href="/tag.html#spark">spark</a>
        
        </div>
        <div id="article-content">
            <p>在spark的实际使用中，生成的结果集可能还会做为其他流程的数据源，进行再次分析处理，本文主要提供一个将spark结果集持久化到关系型数据库（postgresql）中的思路，毕竟对于小规模的数据，使用关系型数据库的sql处理展现数据比起nosql数据库会更加方便。</p>
<p>以spark自带的people.json为例，首先用它生成一个dataframe</p>
<pre class="prettyprint linenums lang-JAVA">//people.json
{&quot;name&quot;:&quot;Michael&quot;}
{&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30}
{&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19}
//生成dataframe
val testdata=sqlContext.jsonFile(&quot;people.json&quot;)
</pre>
<p>现在我们有了一个类型为dataframe的testdata，假设这个就是处理到最后的结果集，那么如何将它保存到数据库呢？一个比较简单的想法是轮询testdata的所有rows，然后将每一个row中的值插入到数据库。</p>
<p>但是带来一个问题是，假如结果集很大，那这个单线程方案的效率就比较值得商榷了，最后的瓶颈很可能是出现在最后的数据库插入上。所以要想办法将其并行。</p>
<p>于是我想到spark的数据处理本身就是并行的，最后的结果集是按照分区分开生成，所以我们可以不用等最后完整的结果集生成之后再去存数据库，而是可以在每一个分区数据生成时就插数据，从而实现了存数据库的并行。 </p>
<p>但是在此之前，我们还需要考虑并行度的问题。因为数据库的连接数是有限制的，而RDD的最后分区数可能很大，假如一个分区就有一个数据库连接的话，最后会导致连接数不足而插入失败。所以在插入数据库之前，我们首先要修改RDD的分区数，保证它在一个合理的范围内，一般来说50个左右。</p>
<p>关于如何修改分区个数，在前一篇<a href="http://zxdy.github.io/articles/spark-job-real-play.html">spark job物理执行图实战</a>中，已经提到过修改分区个数的方法<strong>repartition</strong>，在此就不做过多介绍了。</p>
<p>下面是个简单的demo。要注意几点</p>
<ol>
<li>demo中首先将testdata的分区数修改为4，然后调用mapPartitions对每个分区执行saveToPostgres方法，所以最终会有4个数据库连接。</li>
<li><p>为什么有个collect()尾巴:因为mapPartitions只是个Transformation，而spark application job的触发是需要通过action来完成，光有mapPartitions还不够。</p>
</li>
<li><p>这个demo比较简陋，没有考虑异常处理情况，在本文所用的数据源people.json中，有一行没有age，所以会抛空指针错误，要想正常运行的话，需要在people.json中补上age值。</p>
</li>
<li>记得先在postgresql上新建一个对应的表</li>
<li>别忘了jdbc包</li>
</ol>
<p><strong>代码：</strong></p>
<pre><code class="prettyprint linenums lang-java">def main(args: Array[String]) {
    ....
    val testdata=sqlContext.jsonFile(&quot;people.json&quot;)
    testdata.repartition(4).mapPartitions(saveToPostgres).collect()
}

private def saveToPostgres(rows:Iterator[Row]): Iterator[Row] ={
    val url = &quot;jdbc:postgresql://localhost/postgres&quot;
    val user = &quot;postgres&quot;
    val password = &quot;1234&quot;
    val con = DriverManager.getConnection(url, user, password)
    val sqlstr = &quot;INSERT INTO public.people(age,name) VALUES(?,?)&quot;
    val stmt = con.prepareStatement(sqlstr)
    rows.foreach(insertRow(_,stmt))
    rows
  }

private def insertRow(row:Row,stmt:PreparedStatement): Unit ={
    var age=row.get(0).toString.toInt
    var name=row.get(1).toString
    stmt.setInt(1,age)
    stmt.setString(2,name)
    stmt.executeUpdate()
}
</code></pre>
<p>最后</p>
<p>还有没有更好的方法？从效率上讲，导数据到数据库最快的应该是通过批量load的方法（postgresql的copy api），像demo中的方式虽然做到了并行，但是本质还是一条条记录插入，速度肯定不如批量从文本导入。spark的saveAsTextFile可以用于生成文本文件，但它是一个action方法，所以要等到文本生成完才能导数据，这样一来的话，之后的事情其实跟spark已经关系不大了。只要新写个程序，将spark生成的文本拆分然后并行load即可。个人感觉，还是推荐这种方式，耦合性低，扩展性好，效率也高，而且对于异常数据的处理也都可以交给jdbc完成。</p>

        </div>
        <!-- 多说评论框 start -->
<div class="ds-thread" data-thread-key="how-to-save-rdd-to-postgresql" data-title="如何保存spark结果到postgreSQL" data-url="http://blog.codinglabs.org/articles/how-to-save-rdd-to-postgresql.html"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"zxdy8812"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0]
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end -->

    </div>
</div>
        <div id="footer">
            <div id="footer-inner">
                <p id="copyright">Copyright (c) 2011-2014 Ario</p>
            </div>
        </div>
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        }
    });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

    </body>
</html>

