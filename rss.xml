<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>RSS feed title</title>
        <link>Homepage link</link>
        <description>RSS feed description</description>
        <lastBuildDate>Tue, 21 Apr 2015 20:36:43 +0800</lastBuildDate>
        <language>zh-cn</language>
        
        <item>
            <title>Cassandra partition key, composite key 和 clustering key 的区别</title>
            <link>Homepage link/articles/cassandra-key.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link>
            <guid>Homepage link/articles/cassandra-key.html</guid>
            <author>zxdy@vip.qq.com Ario</author>
            <pubDate>Thu, 12 Mar 2015 00:00:00 +0800</pubDate>
            <description>&lt;p&gt;在Cassandra中，primary key是一个非常重要的概念。关系型数据库中表可以没有primary key（主键），但是Cassandra中建表时必须指定primary key，它不仅决定了表的结构，而且还对数据查询方式的差异有巨大影响。partition key, composite key 和 clustering key共同组成了Cassandra的primary key。&lt;/p&gt;
&lt;p&gt;为了说明它们的不同，我们先来看三张表。&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;create table table1 (
pri_key text PRIMARY KEY,
data text
);

create table table2 (
  key_part_one text,
  key_part_two int,
  data text,
  PRIMARY KEY(key_part_one, key_part_two)
);

create table table3 (
  key_part_one text,
  key_part_two int,
  key_clust_one text,
  key_clust_two int,
  key_clust_three uuid,
  data text,
  PRIMARY KEY((key_part_one,key_part_two), key_clust_one, key_clust_two, key_clust_three)
);
&lt;/pre&gt;
&lt;h2 id=&quot;格式区别&quot;&gt;格式区别&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;table1&lt;/strong&gt;是初级版，primary key最简单的定义方式就是这样。此时pri_key就是partition key，clustering key。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;table2&lt;/strong&gt;是table1的升级版，格式为（key1，key2，key3，...）。key_part_one, key_part_two共同组成了primary key，这种方式即所谓的composite key（组合键）。其中key_part_one是partition key，key_part_two共同组成了primary是clustering key。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;table2&lt;/strong&gt;是高级版。这个primary key的组成最为复杂，格式为（（key1，key2），key3，key4，...）。其中key_part_one,key_part_two 共同组成了partition key。而外边的key_clust_one，key_clust_two, key_clust_three都是clustering key。&lt;/p&gt;
&lt;h2 id=&quot;实际的用途&quot;&gt;实际的用途&lt;/h2&gt;
&lt;p&gt;Partition Key ：决定了数据在Cassandra各个节点的是如何分区的。&lt;/p&gt;
&lt;p&gt;Clustering Key ： 用于在各个分区内的排序。&lt;/p&gt;
&lt;p&gt;Primary Key ： 主键，决定数据行的唯一性&lt;/p&gt;
&lt;p&gt;Composite Key ：只是一个多字段组合的概念&lt;/p&gt;
&lt;p&gt;跟关系型数据库一样，分区都是为了解决大数据量查询的效率问题，所不同的是Cassandra的分区分布在各个节点上。注意同一个分区的数据是在同一个物理节点上的，这就造成一个问题，假如分区内的数据量过大的话，会造成Cassandra读取负载的不均衡，可以用类似于table3的建表方式，多个字段共同组成一个partition key减小单个分区的大小，使各个分区能够更均匀地分布在节点上，从而实现负载均衡。&lt;/p&gt;
&lt;p&gt;但是，问题又来了, 在查询时，cassandra查询的where语句中必须指定所有的partition key，这是最基本的条件。比如:&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-SQL&quot;&gt;PRIMARY KEY((key1，key2)，key3，key4，...)
&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;合法的查询(不考虑二级索引)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where key1 and key2&lt;/p&gt;
&lt;p&gt;where key1 and key2 and key3&lt;/p&gt;
&lt;p&gt;where key1 and key2 and key3 and key4&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非法的查询&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where key1&lt;/p&gt;
&lt;p&gt;where key2&lt;/p&gt;
&lt;p&gt;where key1 and key2 and key4&lt;/p&gt;
&lt;p&gt;所以parition key字段过多会对以后的查询造成很大困扰，在建表的时候首先一定要考虑好数据模型，以免后期掉坑。此外假如与spark集成的话，可以在一定程度上规避掉上面非法查询的问题，通过sparksql可以近似实现关系型数据库sql的查询，而不用考虑查询中一定要带上所有partition key字段。&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>oracle性能优化-CPU篇(下)</title>
            <link>Homepage link/articles/oracle-tunning-cpu-2.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link>
            <guid>Homepage link/articles/oracle-tunning-cpu-2.html</guid>
            <author>zxdy@vip.qq.com Ario</author>
            <pubDate>Tue, 10 Mar 2015 00:00:00 +0800</pubDate>
            <description>&lt;p&gt;接上篇 &lt;a href=&quot;http://zxdy.github.io/articles/oracle-tunning-cpu-1.html&quot;&gt;oracle性能优化-CPU篇(上)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如何有效利用好CPU，主要在于如何写好SQL语句， 并优化数据库内部处理。&lt;/p&gt;
&lt;h2 id=&quot;1-sql语句重用&quot;&gt;1. sql语句重用&lt;/h2&gt;
&lt;h3 id=&quot;11-硬解析与软解析&quot;&gt;1.1 硬解析与软解析&lt;/h3&gt;
&lt;p&gt;相信用过oracle的人，特别是用oracle做后端数据库开发的程序猿都听说过硬解析和软解析。&lt;/p&gt;
&lt;p&gt;当一个sql语句提交后，oracle会首先检查一下共享缓冲池（shared pool）里有没有与之&lt;strong&gt;完全相同&lt;/strong&gt;的语句，如果有的话只须执行软分析即可，否则就得进行硬分析。&lt;/p&gt;
&lt;p&gt;硬解析之所以坑爹的原因是，它需要经解析,制定执行路径,优化访问计划等许多的步骤。不但耗费大量的cpu，更重要的是会占据重要的们闩（latch）资源，严重的影响系统的规模的扩大（即限制了系统的并发行），而且引起的问题不能通过增加内存条和cpu的数量来解决。&lt;/p&gt;
&lt;p&gt;看一眼AWR报表，检查是不是有很多硬解析。下图的硬解析数和 time model statistics的hard parse elapsed time对应，可知该系统是否 是 解析敏感&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/hard_prase.png&quot; alt=&quot;hard parse&quot;&gt;&lt;/p&gt;
&lt;p&gt;有时候当我们在oracle上对某些sql进行测试时，会发现第一次执行的sql比较慢，再次执行往往都比较快，这不仅仅是因为所需要的数据块已经读取到buffer cache中了，还因为再次执行的sql沿用的是上次执行的执行计划，并没有重新做解析。&lt;/p&gt;
&lt;p&gt;因此假如我们需要测试相同sql不同的执行计划时，最好刷新一下当前session的shared pool。注意最好不要全局刷新，特别是在生产环境，这样会导致所有的sql都进行重新解析，可能会严重影响性能。&lt;/p&gt;
&lt;h3 id=&quot;12-绑定变量&quot;&gt;1.2 绑定变量&lt;/h3&gt;
&lt;p&gt;绑定变量的实质就是用于替代sql语句中的常量的替代变量，它能够使得每次提交的sql语句都完全一样。还记得前面说的&quot;oracle会首先检查一下共享缓冲池（shared pool）里有没有与之&lt;strong&gt;完全相同&lt;/strong&gt;的语句&quot;没有？
类似这样：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/bind_var.png&quot; alt=&quot;bind var&quot;&gt;&lt;/p&gt;
&lt;p&gt;一般来说，在实际的开发场景，只要使用比较成熟的数据持久层框架例如mybatis等，基本都可以避免这种因为没有用绑定变量而产生的性能问题。&lt;/p&gt;
&lt;h3 id=&quot;13-索引优化&quot;&gt;1.3 索引优化&lt;/h3&gt;
&lt;p&gt;索引优化这是大坑。。有空新开一篇再写吧&lt;/p&gt;
&lt;h2 id=&quot;2-表连接&quot;&gt;2. 表连接&lt;/h2&gt;
&lt;p&gt;在CBO（hash join只有在CBO才可能被使用到）模式下，优化器计算代价时，首先会考虑hash join。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hash join的主要资源消耗在于CPU和内存（在内存中创建临时的hash表，并hash计算， Mem访问速度是Disk的万倍以上。）&lt;/li&gt;
&lt;li&gt;Nested Loop资源消耗在磁盘IO和CPU。&lt;/li&gt;
&lt;li&gt;sort merge的资源消耗主要在于磁盘IO（扫描表或索引）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相对来说，比较常见的还是hash join以及nested loop。&lt;/p&gt;
&lt;p&gt;表连接科普参见 &lt;a href=&quot;http://blog.csdn.net/tianlesoftware/article/details/5826546&quot;&gt;多表连接的三种方式&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;22-hash-join&quot;&gt;2.2 hash join&lt;/h3&gt;
&lt;p&gt; Hash join的工作方式是将一个表（通常是小一点的那个表）做hash运算，将列数据存储到hash列表中，从另一个表中抽取记录，做hash运算，到hash 列表中找到相应的值，做匹配。&lt;/p&gt;
&lt;h3 id=&quot;22-nested-loop&quot;&gt;2.2 nested loop&lt;/h3&gt;
&lt;p&gt;Nested loops 工作方式是从一张表中读取数据，访问另一张表（通常是索引）来做匹配，nested loops适用的场合是当一个关联表比较小的时候，效率会更高。&lt;/p&gt;
&lt;h3 id=&quot;22-sort-merge&quot;&gt;2.2 sort merge&lt;/h3&gt;
&lt;p&gt;Merge Join 是先将关联表的关联列各自做排序，然后从各自的排序表中抽取数据，到另一个排序表中做匹配，因为merge join需要做更多的排序，所以消耗的资源更多。 通常来讲，能够使用merge join的地方，hash join都可以发挥更好的性能。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;总的来说，小表和大表连接用nested loop，大表和大表连接用hash join，关联列已经排序好的表连接推荐用sort merge（用的不多就是了）。
但凡事总有例外，具体的执行效果还是要根据具体情况看，有时候CBO推荐的执行计划未必是最好的， 此时你可以尝试使用hint或者修改sql语句来改变表连接。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        </item>
        
        <item>
            <title>oracle性能优化-CPU篇(上)</title>
            <link>Homepage link/articles/oracle-tunning-cpu-1.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link>
            <guid>Homepage link/articles/oracle-tunning-cpu-1.html</guid>
            <author>zxdy@vip.qq.com Ario</author>
            <pubDate>Wed, 04 Mar 2015 00:00:00 +0800</pubDate>
            <description>&lt;p&gt;在任何一样计算机软件产品中，当我们需要考虑其性能的时候，往往都会从CPU，IO，Network这三个方面考虑。CPU代表着处理问题的能力，IO代表着存储的吞吐能力，Network代表着数据传输的能力。oracle当然也不例外。&lt;/p&gt;
&lt;p&gt;下图反映的是一个应用程序总体的响应时间的分布情况。用户在前端发出数据的请求之后，经过网络层，到达数据库服务器。数据库服务器接收请求，然后对SQL进行语法语义分析，然后生成执行计划，接着执行sql，取得数据最后再次经过网络层返回到前端展现给用户。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/bottle-neck-of-oracle.png&quot; alt=&quot;bottle-neck-of-oracle&quot;&gt;&lt;/p&gt;
&lt;p&gt;很显然 &lt;strong&gt;oracle的处理时间=cpu处理时间+[资源等待时间+Disk IO 等待时间]&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;根据以上公式，我们可以发现只要能有效地利用cpu资源和降低等待时间就可以提高数据库的性能，使它处理得更快。&lt;/p&gt;
&lt;p&gt;这篇文章主要关注oracle sql在cpu上的性能优化。个人感觉相比等待上的优化简单一些，等待很多时候涉及到并发，资源争用，数据块等知识，优化也更加复杂不好下手。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;那么问题来了：&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-首先，怎么查看cpu信息&quot;&gt;1. 首先，怎么查看CPU信息&lt;/h2&gt;
&lt;p&gt;CPU的多少在很大程度上（质量也是很重要滴）决定了数据库性能的好坏。越多的CPU，可以并发的数目就越多。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用系统命令查看。这里我默认认为oracle是安装在linux服务器上，当然装在windows上的不是没有，只是略奇葩了。&lt;/p&gt;
&lt;p&gt;  refer to  &lt;a href=&quot;http://zxdy.github.io/articles/linux-info-check.html&quot;&gt;查看cpu信息&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;sql查询NUM_CPUS字段。v$osstat这张表包含了很多有用的信息
附上&lt;a href=&quot;http://docs.oracle.com/cd/E11882_01/server.112/e40402/dynviews_2085.htm#REFRN30321&quot;&gt;官方文档&lt;/a&gt;解释&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;SELECT * FROM v$osstat;
&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;awr报告。在awr中会有cpu相关的各种报告，包括硬件信息以及更重要的性能信息。关于cpu的性能分析我会在后面展开。下面这张图展现了当前实例使用的cpu硬件信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/awr_cpu.png&quot; alt=&quot;awr cpu&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;CPUs: 逻辑cpu数
Cores: cpu核数
Sockets: 物理cpu数&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;2-怎么看cpu的性能&quot;&gt;2. 怎么看CPU的性能&lt;/h2&gt;
&lt;p&gt;上面的CPU硬件信息只是帮助我们有个初步的概念，如果说你的数据库性能很差，CPU又很烂，很烂还没几个，那你真的该先换CPU了。。&lt;/p&gt;
&lt;p&gt;换完CPU，我们先来了解下下面这两个概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;host cpu&lt;/li&gt;
&lt;li&gt;instance cpu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在AWR报告中会有这样两个不同的分类，像是这样&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/awr_cpu.png&quot; alt=&quot;host cpu&quot;&gt;&lt;/p&gt;
&lt;p&gt;还有这样&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/instance_cpu.png&quot; alt=&quot;instance cpu&quot;&gt;&lt;/p&gt;
&lt;p&gt;它们其实是分别代表了服务器cpu的负载和oracle实例的负载情况。&lt;/p&gt;
&lt;p&gt;对于host cpu：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&quot;Load Average&quot; begin/end值代表CPU的大致运行队列大小。上图中快照开始
到结束，平均 CPU负载减少了。&lt;/li&gt;
&lt;li&gt;%User+%System=&gt; 总的CPU使用率，在这里是5.7%。&lt;/li&gt;
&lt;li&gt;Busy Time=Elapsed Time * NUM_CPUS * CPU utilization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于instance cpu:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;%Total CPU,该实例所使用的CPU占总CPU的比例 -&gt; % of total CPU for
Instance&lt;/li&gt;
&lt;li&gt;%Busy CPU，该实例所使用的Cpu占总的被使用CPU的比例 -&gt; % of busy CPU for Instance。例如共4个逻辑CPU，其中3个被完全使用， 3个中的1 个完全被该实例使用，则%Total CPU= 1/4 =25%，而%Busy CPU= 1/3= 33%&lt;/li&gt;
&lt;li&gt;当CPU高时一般看%Busy CPU可以确定CPU到底是否是本实例消耗的，还是
主机上其他程序。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;身为一个开发我还要时不时看awr报告也是蛮拼的。一般来讲，awr更多的是展现数据库整体上的性能分析，你可能在以上的host cpu以及instance cpu上发现cpu的负载很高，但这又有什么用呢？是不是觉得没法继续了呢？当然不是，awr还提供了更多的详细的报告帮助我们定位哪些sql的cpu占用比较厉害：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;AWR SQL ordered by Elapsed Time：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/sql_order_by_elapsed_time.png&quot; alt=&quot;AWR SQL ordered by Elapsed Time&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;%CPU - CPU Time as a percentage of Elapsed Time -&gt; 这个语句耗费的DB TIME里CPU TIME占多少比例 -&gt; 这个语句是否是CPU敏感的&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;2.AWR SQL ordered by CPU Time：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/sql_order_by_cpu_time.png&quot; alt=&quot;AWR SQL order by cpu time&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;第一列CPU TIME统计这个sql总共在快照时间内总共花费的cpu时间，在这个值比较高的情况下，如果相应的%CPU值也很高， 说明这个sql在cpu上的负载很高，需要考虑优化。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;如果发现有比较突出的sql，很可能就是瓶颈所在，可以继续跑个@?/rdbms/admin/awrsqrpt.sql看看&lt;/strong&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>oracle性能优化-执行计划篇</title>
            <link>Homepage link/articles/oracle-tunning-sqlplan.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link>
            <guid>Homepage link/articles/oracle-tunning-sqlplan.html</guid>
            <author>zxdy@vip.qq.com Ario</author>
            <pubDate>Mon, 09 Feb 2015 00:00:00 +0800</pubDate>
            <description>&lt;h1 id=&quot;执行计划？&quot;&gt;执行计划？&lt;/h1&gt;
&lt;p&gt;执行计划是sql在数据库中最终的执行路径，包括索引的扫描，数据的读取，过滤，连接，排序等等一系列过程。就像平常在生活中你为了完成一个任务，需要经过很多步骤，如果没有很好的统筹规划，任务时间会不断延长。所以sql的效率跟它的执行计划息息相关，同一个sql可能会有很多不一样的执行计划，基于CBO的oracle会在其中挑选出它认为最快的执行计划进行执行。下面这个流程图展现了优化器选择执行计划过程：&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;http://zxdy-blog.qiniudn.com/sql%20plan.png&quot; alt=&quot;优化器选择执行计划过程&quot;&gt;&lt;/p&gt;
&lt;p&gt; 从上面的流程图可以知道一个高效良好的执行计划需要考虑的因素有&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;绑定变量。//还记得CPU优化篇的sql重用吗&lt;/li&gt;
&lt;li&gt;统计信息。oracle默认会在每天的晚上以及周末触发自动收集数据改变量在10%以上的表的统计信息。如果统计信息过于陈旧，DBA或者开发也可以手动提交收集统计信息任务。&lt;/li&gt;
&lt;li&gt;hint。hint会强制指定执行计划的路径，比如select /*+ use_hash*/ ,不管优化器怎么认为表连接应该使用nested loop更加效率，都会强制使用hash连接&lt;/li&gt;
&lt;li&gt;sql profile。&lt;a href=&quot;https://docs.oracle.com/database/121/TGSQL/tgsql_profiles.htm#TGSQL599&quot;&gt;定义&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sql plan。&lt;a href=&quot;https://docs.oracle.com/database/121/TGSQL/tgsql_spm.htm#TGSQL617&quot;&gt;sql profile vs plan baseline&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;怎么查看执行计划&quot;&gt;怎么查看执行计划&lt;/h1&gt;
&lt;h2 id=&quot;1-explain-plan-for-sql&quot;&gt;1. Explain Plan For SQL&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;不实际执行SQL诧句，生成的计划未必是真实执行的计划 &lt;/li&gt;
&lt;li&gt;必须要有plan_table&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;prettyprint linenums lang-SQL&quot;&gt;EXPLAIN PLAN FOR
  SELECT object_id FROM dba_objects;
SELECT PLAN_TABLE_OUTPUT FROM TABLE(DBMS_XPLAN.DISPLAY());
&lt;/pre&gt;
&lt;h2 id=&quot;2-sqlplus-autotrace&quot;&gt;2. SQLPLUS AUTOTRACE&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;除set autotrace traceonly explain外均实际执行SQL，但仍未必是真实计划&lt;/li&gt;
&lt;li&gt;必须要有plan_table&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;3-其他第三方工具&quot;&gt;3. 其他第三方工具&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;sqldeveloper&lt;/li&gt;
&lt;li&gt;toad（收费，比sqldeveloper强大，细节丰富）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;4-最靠谱的方法&quot;&gt;4. 最靠谱的方法&lt;/h2&gt;
&lt;p&gt;为什么上文说未必是真实的计划呢？这里的指的是实际生产环境使用的执行计划和你手动用上文的方法查看某个sql的执行计划是有可能有差异的。所以最靠谱的办法是在生产环境找到已经执行过的sql或者是正在执行的sql的sql id，然后根据这个id去查看他的执行计划&lt;/p&gt;
&lt;h3 id=&quot;查询历史sql和sql-id&quot;&gt;查询历史sql和sql id&lt;/h3&gt;
&lt;pre class=&quot;prettyprint linenums lang-SQL&quot;&gt;select sql_id,sql_text from v$SQL Where
sql_text not like &#39;%like%&#39;
and sql_text like &#39;%$SQL%&#39;;
--或者
select b.SQL_TEXT,b.FIRST_LOAD_TIME,b.SQL_FULLTEXT
from v$sqlarea b
where b.FIRST_LOAD_TIME between &#39;2014-10-15/09:24:47&#39; and
&#39;2014-10-15/09:24:47&#39; order by b.FIRST_LOAD_TIME
&lt;/pre&gt;
&lt;h3 id=&quot;查询正在执行的sql和sql-id&quot;&gt;查询正在执行的sql和sql id&lt;/h3&gt;
&lt;pre class=&quot;prettyprint linenums lang-SQL&quot;&gt;select a.username, a.sid,b.SQL_TEXT, b.SQL_FULLTEXT,b.sql_id
from v$session a, v$sqlarea b 
where a.sql_address = b.address ;
&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;note:&lt;/p&gt;
&lt;p&gt;V\$SQL, V\$SQLAREA, V\$SQLTEXT三个视图的区别:&lt;/p&gt;
&lt;p&gt;V\$SQL：CHILD CURSOR DETAILS FOR V\$SQLAREA&lt;/p&gt;
&lt;p&gt;V\$SQLAREA：SHARED POOL DETAILS FOR STATEMENTS/ANONYMOUS BLOCKS&lt;/p&gt;
&lt;p&gt;V\$SQLTEXT：SQL TEXT OF STATEMENTS IN THE SHARED POOL&lt;/p&gt;
&lt;p&gt;V\$SQL的每一行表示的是每一个SQL语句的一个版本，而V\$SQLAREA存放的是相同语句不同版本一个GROUP BY汇总。&lt;/p&gt;
&lt;p&gt;V\$SQL及V\$SQLAREA存放着统计信息在调优时使用居多，但其SQL是不全的，如果想获得完整的SQL需使用V\$SQLTEXT。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;精确查询详细执行计划&quot;&gt;精确查询详细执行计划&lt;/h3&gt;
&lt;pre class=&quot;prettyprint linenums lang-SQL&quot;&gt;alter session set STATISTICS_LEVEL = ALL; --不设置无法获得A-ROWS等信息,A-Rows 是实际执行时返回的行数
select * from table(dbms_xplan.display_cursor(&#39;sql_id&#39;,null,&#39;ADVANCED ALLSTATS LAST PEEKED_BINDS&#39;));
&lt;/pre&gt;
&lt;h1 id=&quot;执行计划的执行顺序&quot;&gt;执行计划的执行顺序&lt;/h1&gt;
&lt;p&gt;toad看这个最方便，可以直接显示执行顺序。如图中的12345678就是：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/sqlplan_toad.png&quot; alt=&quot;sql plan by toad&quot;&gt;&lt;/p&gt;
&lt;p&gt;但是如果没有toad的话，通常显示的结果会是这样，要注意的是这里的id并不是顺序：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/sqlplan_sqldeveloper.png&quot; alt=&quot;sql plan&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Id 分配给执行计划中每一个步骤的一个数字 每个步骤（执行计划中的行，戒树中的节点）代表行源 (row source)。&lt;/li&gt;
&lt;li&gt;Operation 该步骤实施的内部操作名 id=0的operation一般是 SELECT/INSERT/UPDATE/DELETE Statement&lt;/li&gt;
&lt;li&gt;Name 该步骤操作的表戒者索引名&lt;/li&gt;
&lt;li&gt;Rows CBO基亍统计信息估计该操作将返回的行数&lt;/li&gt;
&lt;li&gt;Bytes CBO基亍统计信息估计该操作将返回的字节数&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果你学过二叉树的后序遍历的话，其实也很简单。首先我们先将这个图根据缩进转化成一个树。然后对这个树进行一下后续遍历，这个遍历的顺序（4 -&gt; 6 -&gt; 5-&gt; 3 -&gt; 7 -&gt; 2 -&gt; 8 -&gt; 1 -&gt; 0）就是执行顺序。&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;http://zxdy-blog.qiniudn.com/tree.png&quot; alt=&quot;执行计划树&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;如何通过执行计划优化sql&quot;&gt;如何通过执行计划优化sql&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;在计划中，驱动表具有最强的过滤性。&lt;/li&gt;
&lt;li&gt;每个步骤的联接顺序都可保证返回给下一步的行数最少（即，联接顺序应使系统转到尚未使用的最强过滤器）。&lt;/li&gt;
&lt;li&gt;就返回的行数而言，相应的联接方法是适合的。例如，返回的行很多时，使用索引的嵌套循环联接可能不是最佳方法。&lt;/li&gt;
&lt;li&gt;高效地使用视图。查看 SELECT 列表，确定访问的视图是否必需。&lt;/li&gt;
&lt;li&gt;是否存在预料之外的笛卡尔积（即使对于小表，也是如此）。&lt;/li&gt;
&lt;li&gt;高效地访问每个表：考虑 SQL 语句中的谓词和表的行数。查找可疑活动，例如对行数很多的表执行全表扫描（在 WHERE 子句中有谓词）。而对于小表，或根据返回的行数利用更好的联接方法（例如 hash_join）时，全表扫描也许更有效。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参考：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://t.askmaclean.com/thread-3237-1-1.html&quot;&gt;Maclean Liu的Oracle性能优化讲座 真正读懂Oracle SQL执行计划&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.itpub.net/22664653/viewspace-701711/&quot;&gt;如何清除某条SQL在库缓存中的执行计划：dbms_shared_pool.purge &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://files.cnblogs.com/files/kerrycode/ORACLE_11g_ARCHITECTURE.pdf&quot;&gt;oracle 11g 架构图&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
        <item>
            <title>大文件读写效率比较</title>
            <link>Homepage link/articles/file-read-write-compare.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link>
            <guid>Homepage link/articles/file-read-write-compare.html</guid>
            <author>zxdy@vip.qq.com Ario</author>
            <pubDate>Thu, 15 Jan 2015 00:00:00 +0800</pubDate>
            <description>&lt;p&gt;之前做到一个大日志文件（size &gt; 1G）解析的项目，在此记录下对于大文本解析方式的效率比较。不同方式的性能差别很大，那个项目的日志解析时间能从原来的超过36小时优化到只需要2分钟，awk功不可没。&lt;/p&gt;
&lt;h2 id=&quot;bash-比较&quot;&gt;bash 比较&lt;/h2&gt;
&lt;p&gt;bash脚本中对于文本的读取主要有以下四种，尽管 AWK 具有完全属于其本身的语法，但在此我也把它归在一起：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;#方法一
func1(){
    rm -f $2
    echo &quot;$(date) start to read&quot;
    start_time=$(date +%s)
    cat $1|while read Line
    do
        echo $Line &gt;&gt; $2
    done
    end_time=$(date +%s)
    echo &quot;$(date) end to read&quot;
    echo &quot;cost: &quot;$((end_time-start_time))&quot;sec&quot;
}

#方法二
func2(){
    rm -f $2
    echo &quot;$(date) start to read&quot;
    start_time=$(date +%s)
    while read Line
    do
        echo $Line &gt;&gt; $2
    done &lt;$1
    end_time=$(date +%s)
    echo &quot;$(date) end to read&quot;
    echo &quot;cost: &quot;$((end_time-start_time))&quot;sec&quot;
}

#方法三
func3(){
    rm -f $2
    echo &quot;$(date) start to read&quot;
    start_time=$(date +%s)
    for Line in `cat $1`
    do
        echo $Line &gt;&gt; $2
    done
    end_time=$(date +%s)
    echo &quot;$(date) end to read&quot;
    echo &quot;cost: &quot;$((end_time-start_time))&quot;sec&quot;
}

#func4
func4(){
    rm -f $2
    echo &quot;$(date) start to read&quot;
    start_time=$(date +%s)
    awk &#39;{print $0}&#39; $1 &gt; $2
    echo &quot;$(date) end to read&quot;
    echo &quot;cost: &quot;$((end_time-start_time))&quot;sec&quot;
}


source=$1
dest=$2

#比较结果：
echo &quot;####cat read: &quot;
func1 $source $dest
echo &quot;####redirect read: &quot;
func2 $source $dest
echo &quot;####for read: &quot;
func3 $source $dest
echo &quot;####awk read: &quot;
func4 $source $dest
&lt;/pre&gt;
&lt;p&gt;结果:&lt;/p&gt;
&lt;p&gt;cat read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thu Jan 15 07:57:50 GMT 2015 start to read&lt;/p&gt;
&lt;p&gt;Thu Jan 15 07:58:33 GMT 2015 end to read&lt;/p&gt;
&lt;p&gt;cost: 43sec&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;redirect read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thu Jan 15 07:58:33 GMT 2015 start to read&lt;/p&gt;
&lt;p&gt;Thu Jan 15 07:59:01 GMT 2015 end to read&lt;/p&gt;
&lt;p&gt;cost: 28sec&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;for read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thu Jan 15 07:59:01 GMT 2015 start to read&lt;/p&gt;
&lt;p&gt;Thu Jan 15 08:00:00 GMT 2015 end to read&lt;/p&gt;
&lt;p&gt;cost: 59sec&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;awk read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thu Jan 15 08:00:00 GMT 2015 start to read&lt;/p&gt;
&lt;p&gt;Thu Jan 15 08:00:00 GMT 2015 end to read&lt;/p&gt;
&lt;p&gt;cost: 0sec&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;从以上结果可以看出，awk的效率远超其他方法&lt;/p&gt;
&lt;h2 id=&quot;python-比较&quot;&gt;python 比较&lt;/h2&gt;
&lt;p&gt;python 有三种读取文件的方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;read() 会将所有内容读入到一个字符串中&lt;/li&gt;
&lt;li&gt;readline() 每次读取一行&lt;/li&gt;
&lt;li&gt;readlines() 将所有内容按行读取，返回一个列表，列表中每个元素是一个字符串，一个字符串是一行内容&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以从效率上讲， read() 和readlines()会比readline()高，但是同时对内存的要求也比较高，需要能一次性将文件内容放入内存中。但是如果这个文件很大的话，就会影响到程序运行的速度，甚至会导致程序挂掉，此时分行读取或是设置buff_size会是个更好的选择&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-python&quot;&gt;#!/usr/bin/env python
import time
import os
def func1(source,dest):
    os.remove(dest)
    with open(source, &#39;r&#39;) as fr:
        content=fr.read()
    with open(dest,&#39;w&#39;) as fw:
        fw.write(content)
def  func2(source,dest):
    os.remove(dest)
    fw=open(dest,&#39;w&#39;)
    for line in open(source,&#39;r&#39;):
        fw.write(line)
    fw.close
if __name__ == &#39;__main__&#39;:
    from timeit import Timer
    t1=Timer(&quot;func1(&#39;log&#39;,&#39;log1&#39;)&quot;,&quot;from __main__ import func1&quot;)
    t2=Timer(&quot;func2(&#39;log&#39;,&#39;log1&#39;)&quot;,&quot;from __main__ import func2&quot;)
    print &quot;read once: &quot;+str(t1.timeit(1))
    print &quot;read line: &quot;+str(t2.timeit(1))
&lt;/pre&gt;
&lt;p&gt;40M文件5次处理时间：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;read once: 0.308089971542&lt;/p&gt;
&lt;p&gt;read line: 1.17492413521&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;1G文件首次处理时间：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;read once: 8.17146706581&lt;/p&gt;
&lt;p&gt;read line: 4.13687205315&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;1G文件5次处理时间：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;read once: 7.32681894302&lt;/p&gt;
&lt;p&gt;read line: 30.3610920906&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有意思的是，虽然一次性读入内存效率比line by line读取的效率差，但是假如重复处理同一个文件，一次性读取的总体效率反而高，所以python应该做了类似于缓存的机制。所以当我们用python处理大文本文件的时候需要综合考虑服务器内存，文件处理次数来决定使用哪种方式。&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>python的两种计时方法</title>
            <link>Homepage link/articles/python-timeit.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link>
            <guid>Homepage link/articles/python-timeit.html</guid>
            <author>zxdy@vip.qq.com Ario</author>
            <pubDate>Tue, 13 Jan 2015 00:00:00 +0800</pubDate>
            <description>&lt;p&gt;其实我本意是就贴一段代码完事，但又觉得确实有点干巴巴，还是稍微灌点水吧。&lt;/p&gt;
&lt;p&gt;way1和way2分别代表了两种不同的计时方式，同时也展现了两种不同的用途。&lt;/p&gt;
&lt;p&gt;way1是使用了timeit包。Python 社区有句俗语：&quot;Python 自己带着电池。&quot; 别自己写计时框架。Python 2.7 具备一个叫做 timeit 的完美计时工具。看demo代码可以发现，它不仅支持对单个方法计时，还支持方法传参，更支持重复计时。我觉得这个计时方法更加适用于测试，特别是性能测试的时候。&lt;/p&gt;
&lt;p&gt;way2没什么好说的，一种很简单粗暴傻瓜化的计时方法，换种编程语言也是相同的套路。这个计时方法相比way1来说，更加浅显易懂，操作也简单，就是代码量相对较多，不够简洁。但它也不是没有市场，比较适合在python脚本中写日志。&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-python&quot;&gt;#!/usr/bin/env python
import time
from timeit import Timer
def func1(x):
    sum=0
    for i in  xrange(x*10000):
        sum=sum+i

def func2(x,y):
    sum=0
    for i in  xrange((x+y)*10000):
        sum=sum+i

if __name__ == &#39;__main__&#39;:
    #way1
    t1=Timer(&quot;func1(2)&quot;,&quot;from __main__ import func1&quot;)
    t2=Timer(&quot;func2(3,4)&quot;,&quot;from __main__ import func2&quot;)
    print(t1.timeit(1))
    print(t2.timeit(1))
    print(t1.repeat(4,10)) //第一个参数是重复整个测试的次数，第二个参数是每个测试中调用被计时语句的次数

    #way2
    start = time.clock()
    func1(2)
    elapsed = (time.clock() - start)
    print(&quot;Time used:&quot;, elapsed)
&lt;/pre&gt;
</description>
        </item>
        
        <item>
            <title>oracle性能优化-索引篇</title>
            <link>Homepage link/articles/oracle-tunning-index.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link>
            <guid>Homepage link/articles/oracle-tunning-index.html</guid>
            <author>zxdy@vip.qq.com Ario</author>
            <pubDate>Fri, 19 Dec 2014 00:00:00 +0800</pubDate>
            <description>&lt;p&gt;索引是每种数据库都避不开的一个话题。很多人不管是DBA还是开发，对于索引的印象就是：sql慢？加个索引吧。但是为什么加了索引sql就能变快呢？加了索引是不是一定就会变快？怎么加索引效率会最高？&lt;/p&gt;
&lt;h2 id=&quot;从b-tree索引讲起&quot;&gt;从B-tree索引讲起&lt;/h2&gt;
&lt;p&gt;索引就像是一本字典的查询页，你可以通过字母或者是偏旁快速定位需要查询的字。类似通过字母查询的是聚集索引（表中行的物理顺序与键值的逻辑索引顺序相同），反之通过偏旁的是非聚集索引（表中行的物理顺序与键值的逻辑索引顺序不相同）。扯远了，这里要讲的是b-tree索引。&lt;/p&gt;
&lt;p&gt;oracle默认建的索引就是b-tree索引，他的工作方式如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/B-TREE.png&quot; alt=&quot;b-tree&quot;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;很显然索引也是需要占用空间的，只是没有普通数据那么大而已。索引的大小跟数据量以及索引列的多少成正比。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;一般来说，还是推荐将索引使用的空间与数据的tablespace分开来，索引建在索引专用的tablespace中，方便维护，同时也有助于查询效率提高。oracle的数据查询说到底就是数据块的读取，查询索引的时候，也是在读取数据块，索引数据块越集中，查询的速度越快，如果索引的数据块跟普通数据块一起放在同一个tablespace中，肯定相对比较分散，影响查询效率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;b-tree索引顾名思义就是一棵树。当我们要查询字段值为&quot;ACER&quot;的时候，oracle首先会读取索引的根节点，如图中的20号数据块。这个数据块中维护着子节点的信息，通过它可以往下找到子节点30号数据块，这个节点维护着下一个节点信息，在此表示所有首字母A-F的索引信息都是它的子节点。然后找到叶子节点39号数据块。这个数据块中包含索引字段值首字母是A-F的所有值以及相对应的rowid。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;到此，假如你的sql只是需要查到索引的字段值得话，整个过程就到此结束了。但是如果还要对应的其他字段值，则还需通过上一步得到rowid去数据存储的tablespace中找到相应数据块。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;发现什么没有？其实oracle的sql查数据最快的方法是直接用rowid查询啊，连索引扫描的时间都省掉了。  &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;如何挑选索引列&quot;&gt;如何挑选索引列&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;定义一个主键列，oracle会自动为它建一个unique index&lt;/li&gt;
&lt;li&gt;外键列通常是需要建索引的&lt;/li&gt;
&lt;li&gt;在经常用到的列上建索引，特别是在where谓词中经常出现的&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;你大概会说上面三点基本就是废话。。&lt;/p&gt;
&lt;p&gt;好吧，我们说点不是废话的：索引扫描的三种情形&lt;/p&gt;
&lt;h3 id=&quot;1-sql所有需要的数据都在索引上&quot;&gt;1. sql所有需要的数据都在索引上&lt;/h3&gt;
&lt;p&gt;又分为两种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;index range scan.&lt;/strong&gt;
在上图中，假如只要查询&quot;ACER&quot;,那么oracle只需读取3个block，同时在第三个block中挑出是&quot;ACER&quot;的值就可以了,此时用的就是range scan。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;index fast full scan.&lt;/strong&gt;
还是上图，假如需要查询被索引的所有值或是大部分值，oralce优化器会认得此时用 index fast full scan最快，就是不挑了，把索引上的值都扫一遍&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;2-sql需要的数据不都在索引上&quot;&gt;2. sql需要的数据不都在索引上&lt;/h3&gt;
&lt;p&gt;这个情况是最普遍的，即&lt;em&gt;从B-tree索引讲起&lt;/em&gt;的第4点情况，反映在执行计划中就是 TABLE ACCESS BY INDEX ROWID&lt;/p&gt;
&lt;h3 id=&quot;3-不想走索引，就是这么任性&quot;&gt;3. 不想走索引，就是这么任性&lt;/h3&gt;
&lt;p&gt;有时候即使表上有各种索引，oracle优化器还是会认为不走索引反而更快，比如全表扫描（TABLE ACCESS FULL）。&lt;/p&gt;
&lt;p&gt;索引不是越多越好，也不是一个复合索引覆盖的列越多越好，需要综合考虑性能。建了索引之后也要查看一下执行计划，是不是确实用到了索引，
同时要看下sql的buffer read和physical read跟原先比如何。&lt;/p&gt;
&lt;h2 id=&quot;bitmap索引稍及&quot;&gt;bitmap索引稍及&lt;/h2&gt;
&lt;p&gt;我并没有用过bitmap索引，经验不多，所以就只能稍微介绍下。&lt;/p&gt;
&lt;p&gt;bitmap索引即所谓的位图索引，它和b-tree索引最大的区别是适用于只有几个固定值的列，如性别、婚姻状况等，如果列的取值非常多则适用于b-tree索引。&lt;/p&gt;
&lt;p&gt;另外位图索引适合静态数据，而不适合索引频繁更新的列。因为它所造成的行锁比较广，非常坑爹。举个例子，某张表上的性别列上是位图索引，有次将其中某一个人的性别从男改成女时，会将表上所有女性记录加锁，直到commit。可以想象假如更新频繁的话，会出现严重的锁等待事件&lt;/p&gt;
&lt;h2 id=&quot;监控索引使用情况&quot;&gt;监控索引使用情况&lt;/h2&gt;
&lt;p&gt;在一个很大的数据库中，可能会有成百上千个索引分布在各个表上。随着时间的过去，可能其中很多索引都已经用不到了。虽然索引可以帮助加快数据查询的速度，但是相应也是有代价的。当表的数据有插入，更新，删除的变化时，表上的索引也需要更新，他会占用cpu和磁盘的资源，索引越多，占用的资源越多。所以对于这些已经没有在被使用的索引，可以删掉，节约资源。&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-SQL&quot;&gt;--对单独某个索引启用监控
alter index f_regs_idx1 monitoring usage; 
--查看索引使用情况（对于在不同schema下的索引需要登录到相应的用户上去查看）
select index_name, table_name, monitoring, used from v$object_usage; 

--批量启用监控
set pagesize 0 head off linesize 132 
spool enable_mon.sql 
select   
&#39;alter index &#39; || index_name || &#39; monitoring usage;&#39; 
from user_indexes; 
spool off; 

--禁用监控 
alter index f_regs_idx1 nomonitoring usage;
&lt;/pre&gt;
&lt;h2 id=&quot;怎样加快建索引的速度&quot;&gt;怎样加快建索引的速度&lt;/h2&gt;
&lt;p&gt;参见 &lt;a href=&quot;http://zxdy.github.io/articles/speed-up-create-index.html&quot;&gt;oracle 如何加快建立索引的速度
&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        
        <item>
            <title>oracle 如何加快建立索引的速度</title>
            <link>Homepage link/articles/speed-up-create-index.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link>
            <guid>Homepage link/articles/speed-up-create-index.html</guid>
            <author>zxdy@vip.qq.com Ario</author>
            <pubDate>Fri, 19 Sep 2014 00:00:00 +0800</pubDate>
            <description>&lt;p&gt;通常情况下对oracle的表建立索引的时候并不需要考虑效率问题，这个通常情况指的是相应的表数据在百万级以下。但是一旦数据量大到千万级，亿级甚至更大的时候，我们就不能不考虑新建索引的效率问题，因为当表在建立索引的时候，会产生锁阻塞新数据的更新，如果索引不能很快地建立完毕，会对生产环境造成影响。&lt;/p&gt;
&lt;h1 id=&quot;1-pga设置&quot;&gt;1. PGA设置&lt;/h1&gt;
&lt;p&gt;hash_area_size： 这个参数控制每个会话的hash内存空间有多大。它也可以在会话级和实例级被修改。默认值是sort area空间大小的两倍
sort_area_size:  因为排序通常是在PGA中进行的，需要防止因空间或内存不足导致需要disk排序。&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;alter session set workarea_size_policy=manual;
alter session set hash_area_size=100000; 
alter session set sort_area_size=2000000000; -- 在系统可用内存足够的情况下，最大可以到2G
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;question&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;什么是PGA&lt;/li&gt;
&lt;li&gt;什么是SGA&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;2-增加temp表空间&quot;&gt;2. 增加temp表空间&lt;/h1&gt;
&lt;p&gt;因为大表的数据量比较大，导致建索引时需要的temp表空间也比较大，一般来说接近索引的大小，没把握的情况下可以估算一下索引的大小先：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;set serveroutput on
declare  
 v_ddl varchar(1024);  
 v_used_bytes number;  
 v_alloc_bytes number;  
 begin  
 dbms_space.create_index_cost(  
 ddl =&gt;&#39; create index ids_t on user(userid)&#39;,used_bytes=&gt;v_used_bytes,alloc_bytes =&gt;v_alloc_bytes);  
 dbms_output.put_line(&#39;used_bytes=&#39;||v_used_bytes||&#39; bytes&#39;||&#39; alloc_bytes=&#39;|| v_alloc_bytes || &#39; bytes&#39;);  
 end;  
 /
&lt;/pre&gt;
&lt;p&gt;另外在建索引的过程中也可以随时监控表空间的使用情况，一旦发现temp表空间不够，可以随时加大&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;--查询表空间使用情况
SELECT UPPER(F.TABLESPACE_NAME) &quot;表空间名&quot;,
D.TOT_GROOTTE_MB &quot;表空间大小(M)&quot;,
D.TOT_GROOTTE_MB - F.TOTAL_BYTES &quot;已使用空间(M)&quot;,
TO_CHAR(ROUND((D.TOT_GROOTTE_MB - F.TOTAL_BYTES) / D.TOT_GROOTTE_MB * 100,2),&#39;990.99&#39;) &quot;使用比&quot;,
F.TOTAL_BYTES &quot;空闲空间(M)&quot;,
F.MAX_BYTES &quot;最大块(M)&quot;
FROM (SELECT TABLESPACE_NAME,
ROUND(SUM(BYTES) / (1024 * 1024), 2) TOTAL_BYTES,
ROUND(MAX(BYTES) / (1024 * 1024), 2) MAX_BYTES
FROM SYS.DBA_FREE_SPACE
GROUP BY TABLESPACE_NAME) F,
(SELECT DD.TABLESPACE_NAME,
ROUND(SUM(DD.BYTES) / (1024 * 1024), 2) TOT_GROOTTE_MB
FROM SYS.DBA_DATA_FILES DD
GROUP BY DD.TABLESPACE_NAME) D
WHERE D.TABLESPACE_NAME = F.TABLESPACE_NAME
ORDER BY 4 DESC;

select file_name,bytes/1024/1024 &quot;MB&quot;,autoextensible,tablespace_name from dba_temp_files;

--增加表空间大小
alter tablespace USERS add datafile &#39;/opt/ora9/users02.dbf&#39; size 50M autoextend on next 50M maxsize UNLIMITED;
&lt;/pre&gt;
&lt;h1 id=&quot;3-使用并行参数&quot;&gt;3. 使用并行参数&lt;/h1&gt;
&lt;p&gt;关于利用并行度创建索引，前提是多个CPU，单CPU下用并行度创建索引，可能会造成资源的争用。理论上来说8个CPU, 可以用parallel 6 ,最多占用6个CPU,另外留下两个CPU供其他进程使用。
查看CPU核数的方法有很多，详细见（oracle性能优化-CPU篇）。最简单地就是用下面这个sql直接查&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;SELECT * FROM v$osstat where stat_name=&#39;NUM_CPUS&#39;;
&lt;/pre&gt;
&lt;h1 id=&quot;4-使用nologging&quot;&gt;4. 使用nologging&lt;/h1&gt;
&lt;p&gt;nologging, 绝对应该使用，能减少大量redo log，使速度大幅上升。&lt;/p&gt;
&lt;p&gt;于是一个比较标准的并行nologging建索引语句就出炉了。对于生产环境，保险的办法是再加上online参数，避免建索引时的锁对dml产生阻塞。&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;CREATE INDEX  table_idx ON table (col )  NOLOGGING PARALLEL 6;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于一个比较大的操作，oracle可能会有等待事件发生
首先可以通过sql developer查看等待时间的信息，得到等待时间的p1，p2，p3。然后通过下面的sql转换p1，p2得到具体等待的object。&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;select 
   owner,
   segment_name,
   segment_type
from 
   dba_extents
where 
   file_id = &amp;P1 and &amp;P2 between block_id and block_id + blocks -1;
&lt;/pre&gt;
&lt;/blockquote&gt;
</description>
        </item>
        
        <item>
            <title>AWR 启用和导出</title>
            <link>Homepage link/articles/extract-awr.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link>
            <guid>Homepage link/articles/extract-awr.html</guid>
            <author>zxdy@vip.qq.com Ario</author>
            <pubDate>Fri, 08 Aug 2014 00:00:00 +0800</pubDate>
            <description>&lt;h1 id=&quot;1．awr的启用&quot;&gt;1．AWR的启用&lt;/h1&gt;
&lt;p&gt;在默认情况下，Oracle启用数据库统计收集这项功能（即启用AWR）。是否启用AWR由初始化参数STATISTICS_LEVEL控制&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;SQL&gt; SHOW PARAMETER STATISTICS_LEVEL 
NAME                                 TYPE        VALUE
------------------------------------ ----------- ------------------------------
statistics_level                     string      TYPICAL
&lt;/pre&gt;
&lt;p&gt;如果STATISTICS_LEVEL的值为TYPICAL或者 ALL，表示启用AWR；如果STATISTICS_LEVEL的值为BASIC，表示禁用AWR。&lt;/p&gt;
&lt;p&gt;初始化参数statistics_level介绍：&lt;/p&gt;
&lt;p&gt;AWR的行为受到参数STATISTICS_LEVEL的影响。这个参数有三个值：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;BASIC：awr统计的计算和衍生值关闭.只收集少量的数据库统计信息.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;TYPICAL：默认值．只有部分的统计收集.他们代表需要的典型监控oracle数据库的行为.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ALL : 所有可能的统计都被捕捉. 并且有操作系统的一些信息.这个级别的捕捉应该在很少的情况下,比如你要更多的sql诊断信息的时候才使用.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;2-导出awr报告&quot;&gt;2. 导出AWR报告&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;AWR比对报告&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;@$ORACLE_HOME/rdbms/admin/awrddrpt.sql
&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;RAC全局AWR&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;@$ORACLE_HOME/rdbms/admin/awrgrpt.sql
&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;本实例的AWR报告，运行脚本awrrpt.sql。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;@$ORACLE_HOME/rdbms/admin/awrrpt.sql
&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;产生某个实例（RAC）的AWR报告，运行脚本awrrpti.sql。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;@$ORACLE_HOME/rdbms/admin/awrrpti.sql
&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;产生某条SQL语句的AWR报告，运行脚本awrsqrpt.sql。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;@$ORACLE_HOME/rdbms/admin/awrsqrpt.sql
&lt;/pre&gt;
</description>
        </item>
        
        <item>
            <title>Web 日志分析</title>
            <link>Homepage link/articles/web-log-analyze.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link>
            <guid>Homepage link/articles/web-log-analyze.html</guid>
            <author>zxdy@vip.qq.com Ario</author>
            <pubDate>Wed, 23 Apr 2014 00:00:00 +0800</pubDate>
            <description>&lt;h2 id=&quot;1-web业务分析&quot;&gt;1.    Web业务分析&lt;/h2&gt;
&lt;p&gt;所有统计分析基于三维原则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;时间线。所以统计图应有时间线功能。粒度支持到分钟级别。支持时，日，周，月，年切换。支持用户自定义时间&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在时间线基础上针对某块业务进行分析，如访客行为，流量趋势，页面访问等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在以上两个维度上，支持各种基础统计指标（如pv，uv，ip，停留时间等，详见基础统计指标）切换，对比。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;11-综合报告&quot;&gt;1.1.    综合报告&lt;/h3&gt;
&lt;h4 id=&quot;111-网站信息&quot;&gt;1.1.1    网站信息&lt;/h4&gt;
&lt;p&gt;显示网站的基本信息：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;网站名称&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;网站地址&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;开始统计时间&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;已经统计时间&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&quot;112-最近30天流量趋势&quot;&gt;1.1.2    最近30天流量趋势&lt;/h4&gt;
&lt;p&gt;分析从当日至前30日内的PV、UV、IP数据及其变化趋势。系统能根据日，周，月，年切换统计。图表方案可以参照以下形式，所有对比分析的三维数据集图都可沿用以下表现形式，保持一致性，同时支持不同类型的图表切换。&lt;/p&gt;
&lt;p&gt;系统实时更新“网站流量”数据中的“今日”、“昨日此时”和 “预计今日”部分中的数据，帮助客户实时了解网站的流量数据；&lt;/p&gt;
&lt;h4 id=&quot;113-访客粘度指标&quot;&gt;1.1.3    访客粘度指标&lt;/h4&gt;
&lt;p&gt;分别统计分析周，月，年回头客的总量，当前时间区间回头率，平均停留时间，平均访问页数。&lt;/p&gt;
&lt;h3 id=&quot;12-最近访客&quot;&gt;1.2.    最近访客&lt;/h3&gt;
&lt;h4 id=&quot;121-访客概要&quot;&gt;1.2.1    访客概要&lt;/h4&gt;
&lt;p&gt;可以通过访问时间、IP地址、来源地区、来源网址、访问最多的网址等多个维度对访问访客的情况及行为进行分析，页面上显示最多100条的最近访客信息。
以（列表？）形式展现&lt;/p&gt;
&lt;h4 id=&quot;122-访客跟踪&quot;&gt;1.2.2    访客跟踪&lt;/h4&gt;
&lt;p&gt;可以查看到对应访客最近30分钟内访问的全部页面，详细了解访客的访问轨迹（以visit为单位，每次进入到离开的url轨迹）、访客地区、进入时间、停留时间、访问来源，探索访客的行为规律。&lt;/p&gt;
&lt;h3 id=&quot;13-时段分析&quot;&gt;1.3.    时段分析&lt;/h3&gt;
&lt;p&gt;时段分析功能，客户可以自定义对24小时内PV、UV、IP数据及变化趋势进行分析，系统会每分钟对该数据进行更新。&lt;/p&gt;
&lt;p&gt;时段分析功能分为：&lt;/p&gt;
&lt;p&gt;单日流量：可以分析某一天各小时内的PV、UV、IP数据及变化情况。&lt;/p&gt;
&lt;p&gt;多日对比：可以分别分析对比选定日期内PV、UV、IP数据及变化情况。&lt;/p&gt;
&lt;p&gt;时段趋势：可以选择某段日期区间内分析某一小时时段的PV、UV、IP数据及变化情况。&lt;/p&gt;
&lt;h3 id=&quot;14-每日分析&quot;&gt;1.4.    每日分析&lt;/h3&gt;
&lt;p&gt;可以按照客户选择的时间，对于本周、上周、本月、上月或任意时间，访客每日访问网站的状况及访客行为进行分析，系统每分钟对数据进行更新。使客户可以最简单、直接的了解站点一定时期内的流量和访客情况。&lt;/p&gt;
&lt;h4 id=&quot;141-每日流量：&quot;&gt;1.4.1    每日流量：&lt;/h4&gt;
&lt;p&gt;分析当日及30日内PV、UV、IP的数据及变化；&lt;/p&gt;
&lt;h4 id=&quot;142-访客平均停留时间：&quot;&gt;1.4.2    访客平均停留时间：&lt;/h4&gt;
&lt;p&gt;针对访客行为分析，分析选定时间内访客平均停留时间；&lt;/p&gt;
&lt;h4 id=&quot;143-访客平均访问页数：&quot;&gt;1.4.3    访客平均访问页数：&lt;/h4&gt;
&lt;p&gt;针对访客行为分析，分析选定时间内访客平均访问页数；&lt;/p&gt;
&lt;h3 id=&quot;15-来源分析&quot;&gt;1.5.    来源分析&lt;/h3&gt;
&lt;h4 id=&quot;151-ip来源&quot;&gt;1.5.1    ip来源&lt;/h4&gt;
&lt;p&gt;IP来源分析功能，可以记录并分析访问客户网站的IP信息，记录各IP访问的PV量，此功能可以帮助客户更深入的分析来源IP所对应访客的访问行为，及时发现异常或恶意访问的IP。同时，客户可以通过点击页面显示的IP地址查询出该IP所对应的地区名称，IP构成：分析访客IP构成比例(分为直接输入、搜索引擎导入、其他来源导入)。&lt;/p&gt;
&lt;h4 id=&quot;152-访问来源分析&quot;&gt;1.5.2    访问来源分析&lt;/h4&gt;
&lt;p&gt;按照不同时间段
各访问来源流量：针对所有访客访问来源，对PV、UV、IP、平均访问时间、平均访问页数等访客行为特点进行分析，同时可以显示各来源占总来源的数据比例；
指定访问来源流量：针对选择的来源分析该来源带来的流量（PV、UV、IP、平均访问时间、平均访问页数）趋势，&lt;/p&gt;
&lt;h3 id=&quot;16-被访主机分析&quot;&gt;1.6.    被访主机分析&lt;/h3&gt;
&lt;p&gt;按照不同时间段分析&lt;/p&gt;
&lt;p&gt;1.6.1    各主机流量&lt;/p&gt;
&lt;p&gt;对比分析各个主机在PV、UV、IP、平均访问时间、平均访问页数维度上的趋势&lt;/p&gt;
&lt;p&gt;1.6.2    指定主机流量&lt;/p&gt;
&lt;p&gt;指定某主机分析在PV、UV、IP、平均访问时间、平均访问页数维度上的趋势&lt;/p&gt;
&lt;h3 id=&quot;17-被访页面分析&quot;&gt;1.7.    被访页面分析&lt;/h3&gt;
&lt;p&gt;被访页面分析功能，可以按照不同时间段，分别对站内被访问页面流量及趋势进行分析，系统会每小时对该数据进行更新。&lt;/p&gt;
&lt;h4 id=&quot;171-被访页面流量&quot;&gt;1.7.1    被访页面流量&lt;/h4&gt;
&lt;p&gt;用来分析站内被访页面流量比例，并按流量大小排序，目前报表中显示被访最多的100个页面信息；&lt;/p&gt;
&lt;h4 id=&quot;172-指定被访页面流量趋势&quot;&gt;1.7.2    指定被访页面流量趋势&lt;/h4&gt;
&lt;p&gt;可以通过输入访问页面url地址，按时间分析该页面流量变化趋势。&lt;/p&gt;
&lt;h3 id=&quot;18-访问入口分析&quot;&gt;1.8.    访问入口分析&lt;/h3&gt;
&lt;h4 id=&quot;181-各访问入口流量：&quot;&gt;1.8.1    各访问入口流量：&lt;/h4&gt;
&lt;p&gt;针对所有访问入口，统计访客从该入口进入网站的次数，并排序显示；&lt;/p&gt;
&lt;h4 id=&quot;182-指定访问入口流量趋势：&quot;&gt;1.8.2    指定访问入口流量趋势：&lt;/h4&gt;
&lt;p&gt;输入指定的入口链接，选择统计时间段，可显示该时间段内指定访问入口的入口次数及相应百分比&lt;/p&gt;
&lt;h3 id=&quot;19-访问出口分析&quot;&gt;1.9.    访问出口分析&lt;/h3&gt;
&lt;h4 id=&quot;191-各访问出口流量：&quot;&gt;1.9.1    各访问出口流量：&lt;/h4&gt;
&lt;p&gt;针对所有访问出口，统计访客从该出口离开网站的次数，并排序显示；&lt;/p&gt;
&lt;h4 id=&quot;192-指定访问出口流量趋势：&quot;&gt;1.9.2    指定访问出口流量趋势：&lt;/h4&gt;
&lt;p&gt;输入指定的出口链接，选择统计时间段，可显示该时间段内指定访问出口的出口次数及相应百分比。&lt;/p&gt;
&lt;h3 id=&quot;110-客户端分析&quot;&gt;1.10.    客户端分析&lt;/h3&gt;
&lt;p&gt;供客户端时间线分析功能，统计和分析访客浏览器内核、浏览器使用情况。帮助客户更有针对性地设计网站，保证网站前端兼容性。&lt;/p&gt;
&lt;h2 id=&quot;2-基础统计指标&quot;&gt;2.    基础统计指标&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;PV(访问量)：即Page View, 即页面被打开或请求的次数，访客每次刷新即新增一条web日志就被计算一次，web日志的条数代表PV总量。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;另：唯一页面浏览次数UPV。主要是避免页面的重复加载和刷新导致Pageviews虚高的情况，所以在同一个Visit当中重复打开同一个页面，该页面的Unique Pageviews始终只被记为1次。&lt;/p&gt;
&lt;p&gt;定义visit：
同一个访问者（同ip，同浏览器）的两个相邻pageviews之间的时间间隔如果超过了30分钟，会被记录为一个新的visit。
一天结束时，持续的浏览行为自动被切分为两个visit。
一个访问者更换了与原先来源不同的其他来源再次访问这个网站，也会被记为一个新的visit，即使在30分钟内更换也如此。但更换为直接流量除外，即referer字段为空。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;UV(独立访客)：即Unique Visitor,访问客户网站的一台电脑客户端（相同ip相同浏览器）为一个访客。00:00-24:00内相同的客户端只被计算一次。主流应使用cookie计算&lt;/li&gt;
&lt;li&gt;IP(独立IP)：指独立来源IP。00:00-24:00内相同IP地址只被计算一次。&lt;/li&gt;
&lt;li&gt;24小时独立IP：指每小时独立的IP地址。该数据每个小时独立去重。&lt;/li&gt;
&lt;li&gt;最高IP : 指选择时间段范围内（用户指定），访问最多的ip&lt;/li&gt;
&lt;li&gt;最高PV：指选择时间段范围内（用户指定），某页面访问量最高的数值&lt;/li&gt;
&lt;li&gt;新访客：某客户端首次访问（从未访问）为一个新访客。&lt;/li&gt;
&lt;li&gt;最近访客：最近一段时间内访问客户网站的客户端。显示100条。&lt;/li&gt;
&lt;li&gt;常驻访客：指7天内，一个月内，一年visit高于平均值访问网站的访客。&lt;/li&gt;
&lt;li&gt;进入页面：单个访客每一次visit的第一个页面为进入页面。&lt;/li&gt;
&lt;li&gt;离开页面：单个访客每一次visit所查看的最后一个页面为离开页面。&lt;/li&gt;
&lt;li&gt;进入人次：指从某页面进入网站的人次。&lt;/li&gt;
&lt;li&gt;离开人次：指从某页面离开网站的人次。&lt;/li&gt;
&lt;li&gt;进入时间：访客每次发起visit进入网站的时间。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;停留时间：单个访客每次visit从第一个页面到最后离开停留的时间。注：涉及停留时间需要算法支持，主流算法有两种。Google Analytics 采用线性化访问过程的方法。将如下的访问过程线性化，离开的页面停留时间记为0.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://zxdy-blog.qiniudn.com/stay_time.png&quot; alt=&quot;stay&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;按时间顺序转化为&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;http://zxdy-blog.qiniudn.com/stay_time2.png&quot; alt=&quot;stay2&quot;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;平均停留时间：所有访客的访问过程，访问持续时间的平均值。&lt;/li&gt;
&lt;li&gt;平均访问页数：所有访客的访问过程，连续访问页面数的平均值。&lt;/li&gt;
&lt;li&gt;回头率：指7天内，一个月内，一年内常驻访客数占总访客数的百分比。&lt;/li&gt;
&lt;li&gt;跳失率：表示访客只访问了一个页面就离开的访问次数占该页面总访问次数的比例。&lt;/li&gt;
&lt;li&gt;访问来源：访客来访问网站的途径，即访客通过哪种方式进入网站。&lt;/li&gt;
&lt;li&gt;访客跟踪：可以查看到对应访客最近30分钟内访问的全部页面，详细了解访客的访问轨迹、访客地区、进入时间、停留时间，访问来源，探索访客的行为规律。
22．访客地区比例：每个省市访客占总访客人数的百分比，帮助分析访客地域来源。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;3-日志解析&quot;&gt;3.    日志解析&lt;/h2&gt;
&lt;p&gt;以下是一条标准的web访问日志。&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-text&quot;&gt;218.19.140.242 – - [11/Dec/2013:09:31:17 +0800] “GET /query/p/1/s/-1/ver/ver.html HTTP/1.1″ 200 1933 “-” “Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.2.8) Gecko/20100722 Firefox/3.6.8 (.NET CLR 3.5.30729)”
&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;字段&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;字段值&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;字段意义&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;218.19.140.242&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;来源ip地址&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;-&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;-&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;[11/Dec/2013:09:31:17 +0800]&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;访问时间&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;GET /query/p/1/s/-1/ver/ver.html HTTP/1.1&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;访问方法，url地址，http协议版本&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;200&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;由服务器端发送回客户端状态码&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;1933&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;这项表示服务器向客户端发送了多少的字节&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;-&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.2.8) Gecko/20100722 Firefox/3.6.8 (.NET CLR 3.5.30729)&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;记录客户端的浏览器信息&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
        </item>
        
    </channel>
</rss>
