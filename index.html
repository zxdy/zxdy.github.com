<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.1.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">





  
  
  <link rel="canonical" href="http://yoursite.com/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Hexo</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/05/24/unsupported-feature-of-cassandar-spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ario">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2015/05/24/unsupported-feature-of-cassandar-spark/" class="post-title-link" itemprop="url">spark cassandraSql之坑</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2015-05-24 00:00:00" itemprop="dateCreated datePublished" datetime="2015-05-24T00:00:00+08:00">2015-05-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-09 14:43:23" itemprop="dateModified" datetime="2019-04-09T14:43:23+08:00">2019-04-09</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>简单记录一下在使用cassandra与spark集成时踩到的坑</p>
<ol>
<li><p>cassandra建表时注意全部使用小写，否则在使用cassandrasql时会抛错（这不科学。。）</p>
<blockquote>
<p>Exception in thread “main” java.util.NoSuchElementException: key not found:</p>
</blockquote>
</li>
<li><p>尽量不要使用decimal，否则会抛出错误。而使用int或者是double，基本已经够用。 </p>
<blockquote>
<p>Java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.spark.sql.types.Decimal</p>
</blockquote>
</li>
<li><p>不能一次使用多表关联，仅支持两表关联。（在sparksql的测试用例中发现三表关联的例子，但是我在实际使用中始终会报错，为了不引起麻烦还是将多表关联拆分成两两关联进行处理）</p>
</li>
<li><p>条件查询时，谓词列值为null的行不会被查询，必须要加上 is null或is not null</p>
</li>
<li><p>不支持nvl，decode，但是可以用case when 或者if代替。</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cc.cassandraSql("<span class="keyword">select</span> <span class="keyword">case</span> <span class="keyword">when</span> d_value <span class="keyword">is</span> <span class="literal">NULL</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> d_value <span class="keyword">end</span> <span class="keyword">as</span> d_value <span class="keyword">from</span> test.stackoverflow<span class="string">")</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>不支持update。考虑新建dataframe代替。</p>
</li>
<li><p>spark读取cassandra某个表的并行度由”spark.cassandra.input.split.size”决定,这个参数会动态计算最后的分区数,和spark worker节点数无关。但是SparkContext可以被多个线程使用，这意味着同个Spark Application中的Job可以同时提交到Spark Cluster中，所以可以并行读取不同的表减少整体的等待时间，前提是spark有空闲的资源。</p>
</li>
<li><p>不支持类似以下的查询</p>
 <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cc.cassandraSql("<span class="keyword">select</span> a.* <span class="keyword">from</span> test.stackoverflow <span class="keyword">as</span> a<span class="string">")</span></span><br><span class="line"><span class="string">cc.cassandraSql("</span><span class="keyword">select</span> * <span class="keyword">from</span> test.stackoverflow <span class="keyword">as</span> a <span class="keyword">where</span> k_part_two <span class="keyword">in</span>(<span class="keyword">select</span> <span class="keyword">value</span> <span class="keyword">from</span> test.stackoverflow)<span class="string">")</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>sparksql 语句中对子查询和表名尽量设置别名。</p>
</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/05/06/how-to-save-rdd-to-postgresql/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ario">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2015/05/06/how-to-save-rdd-to-postgresql/" class="post-title-link" itemprop="url">如何保存spark结果到postgreSQL</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2015-05-06 00:00:00" itemprop="dateCreated datePublished" datetime="2015-05-06T00:00:00+08:00">2015-05-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-09 14:37:10" itemprop="dateModified" datetime="2019-04-09T14:37:10+08:00">2019-04-09</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在spark的实际使用中，生成的结果集可能还会做为其他流程的数据源，进行再次分析处理，本文主要提供一个将spark结果集持久化到关系型数据库（postgresql）中的思路，毕竟对于小规模的数据，使用关系型数据库的sql处理展现数据比起nosql数据库会更加方便。</p>
<p>以spark自带的people.json为例，首先用它生成一个dataframe</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//people.json</span></span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"Andy"</span>, <span class="string">"age"</span>:<span class="number">30</span>&#125;</span><br><span class="line">&#123;<span class="string">"name"</span>:<span class="string">"Justin"</span>, <span class="string">"age"</span>:<span class="number">19</span>&#125;</span><br><span class="line"><span class="comment">//生成dataframe</span></span><br><span class="line">val testdata=sqlContext.jsonFile(<span class="string">"people.json"</span>)</span><br></pre></td></tr></table></figure>
<p>现在我们有了一个类型为dataframe的testdata，假设这个就是处理到最后的结果集，那么如何将它保存到数据库呢？一个比较简单的想法是轮询testdata的所有rows，然后将每一个row中的值插入到数据库。</p>
<p>但是带来一个问题是，假如结果集很大，那这个单线程方案的效率就比较值得商榷了，最后的瓶颈很可能是出现在最后的数据库插入上。所以要想办法将其并行。</p>
<p>于是我想到spark的数据处理本身就是并行的，最后的结果集是按照分区分开生成，所以我们可以不用等最后完整的结果集生成之后再去存数据库，而是可以在每一个分区数据生成时就插数据，从而实现了存数据库的并行。 </p>
<p>但是在此之前，我们还需要考虑并行度的问题。因为数据库的连接数是有限制的，而RDD的最后分区数可能很大，假如一个分区就有一个数据库连接的话，最后会导致连接数不足而插入失败。所以在插入数据库之前，我们首先要修改RDD的分区数，保证它在一个合理的范围内，一般来说50个左右。</p>
<p>关于如何修改分区个数，在前一篇<a href="http://zxdy.github.io/articles/spark-job-real-play.html" target="_blank" rel="noopener">spark job物理执行图实战</a>中，已经提到过修改分区个数的方法<strong>repartition</strong>，在此就不做过多介绍了。</p>
<p>下面是个简单的demo。要注意几点</p>
<ol>
<li>demo中首先将testdata的分区数修改为4，然后调用mapPartitions对每个分区执行saveToPostgres方法，所以最终会有4个数据库连接。</li>
<li><p>为什么有个collect()尾巴:因为mapPartitions只是个Transformation，而spark application job的触发是需要通过action来完成，光有mapPartitions还不够。</p>
</li>
<li><p>这个demo比较简陋，没有考虑异常处理情况，在本文所用的数据源people.json中，有一行没有age，所以会抛空指针错误，要想正常运行的话，需要在people.json中补上age值。</p>
</li>
<li>记得先在postgresql上新建一个对应的表</li>
<li>别忘了jdbc包</li>
</ol>
<p><strong>代码：</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span> </span>&#123;</span><br><span class="line">    ....</span><br><span class="line">    val testdata=sqlContext.jsonFile(<span class="string">"people.json"</span>)</span><br><span class="line">    testdata.repartition(<span class="number">4</span>).mapPartitions(saveToPostgres).collect()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">saveToPostgres</span><span class="params">(rows:Iterator[Row])</span>: Iterator[Row] </span>=&#123;</span><br><span class="line">    val url = <span class="string">"jdbc:postgresql://localhost/postgres"</span></span><br><span class="line">    val user = <span class="string">"postgres"</span></span><br><span class="line">    val password = <span class="string">"1234"</span></span><br><span class="line">    val con = DriverManager.getConnection(url, user, password)</span><br><span class="line">    val sqlstr = <span class="string">"INSERT INTO public.people(age,name) VALUES(?,?)"</span></span><br><span class="line">    val stmt = con.prepareStatement(sqlstr)</span><br><span class="line">    rows.foreach(insertRow(_,stmt))</span><br><span class="line">    rows</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">insertRow</span><span class="params">(row:Row,stmt:PreparedStatement)</span>: Unit </span>=&#123;</span><br><span class="line">    <span class="keyword">var</span> age=row.get(<span class="number">0</span>).toString.toInt</span><br><span class="line">    <span class="keyword">var</span> name=row.get(<span class="number">1</span>).toString</span><br><span class="line">    stmt.setInt(<span class="number">1</span>,age)</span><br><span class="line">    stmt.setString(<span class="number">2</span>,name)</span><br><span class="line">    stmt.executeUpdate()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后</p>
<p>还有没有更好的方法？从效率上讲，导数据到数据库最快的应该是通过批量load的方法（postgresql的copy api），像demo中的方式虽然做到了并行，但是本质还是一条条记录插入，速度肯定不如批量从文本导入。spark的saveAsTextFile可以用于生成文本文件，但它是一个action方法，所以要等到文本生成完才能导数据，这样一来的话，之后的事情其实跟spark已经关系不大了。只要新写个程序，将spark生成的文本拆分然后并行load即可。个人感觉，还是推荐这种方式，耦合性低，扩展性好，效率也高，而且对于异常数据的处理也都可以交给jdbc完成。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/04/28/spark-job-real-play/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ario">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2015/04/28/spark-job-real-play/" class="post-title-link" itemprop="url">spark job物理执行图实战</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2015-04-28 00:00:00" itemprop="dateCreated datePublished" datetime="2015-04-28T00:00:00+08:00">2015-04-28</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-09 15:23:07" itemprop="dateModified" datetime="2019-04-09T15:23:07+08:00">2019-04-09</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文主要通过一个具体的spark application来讲述spark job执行过程中关于stage划分，stage提交，task运行的流程。主要也是因为上篇的源码阅读只有纯粹的理论，所以希望能通过这篇实战将理论讲的更清楚一点。</p>
<!-- toc -->
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD，全称为Resilient Distributed Datasets，是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。在这些操作中，诸如map、flatMap、filter等转换操作，很好地契合了Scala的集合操作。除此之外，RDD还提供了诸如join、groupBy、reduceByKey等更为方便的操作（注意，reduceByKey是action，而非transformation），以支持常见的数据运算。</p>
<p>RDD作为数据结构，本质上是一个只读的分区记录集合。一个RDD可以包含多个分区（partition），每个分区就是一个dataset片段。RDD可以相互依赖。</p>
<p>如图是一个比较简单的RDD数据流转图，P1,P2 代表各个RDD内的分区。</p>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/RDD.png" alt="RDD"></p>
<p>总结：</p>
<ul>
<li>它是不变的数据结构存储</li>
<li>它是支持跨集群的分布式数据结构</li>
<li>可以根据数据记录的key对结构进行分区</li>
<li>提供了粗粒度的操作，且这些操作都支持分区</li>
<li>它将数据存储在内存中，从而提供了低延迟性</li>
</ul>
<h3 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h3><ul>
<li><p>什么是partition</p>
<p>每个 partition 就是一个RDD的dataset片段，他支持比RDD更细粒度的操作</p>
</li>
<li><p>RDD 中有几个partition</p>
<ul>
<li><p>初始由并行度决定。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val data1=Array(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line">data1: Array[Int] = Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; val rangePairs1 = sc.parallelize(data1, <span class="number">3</span>)</span><br><span class="line">rangePairs1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">23</span></span><br><span class="line">scala&gt; rangePairs1.partitions.size</span><br><span class="line">res2: Int = <span class="number">3</span>   <span class="comment">//设置并行度为3之后，RDD rangePairs1 的partition个数也为3</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>修改partition个数。</p>
<p>下面使用了两种修改partition数量的操作repartition和partitionBy。<br>从DebugString来看，partitionBy的操作代价要小于repartition，但是repartition的适用性比partitionBy广，具体怎么用根据实际情况来吧。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">    <span class="number">1</span>.repartition---</span><br><span class="line">    scala&gt; val hashPairs1 = rangePairs1.repartition(<span class="number">6</span>)</span><br><span class="line">    hashPairs1: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[<span class="number">4</span>] at repartition at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">    </span><br><span class="line">    scala&gt; hashPairs1.partitions.size</span><br><span class="line">    res0: Int = <span class="number">6</span></span><br><span class="line">    </span><br><span class="line">    scala&gt; println(hashPairs1.toDebugString)</span><br><span class="line">(<span class="number">6</span>) MapPartitionsRDD[<span class="number">21</span>] at repartition at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line"> |  CoalescedRDD[<span class="number">20</span>] at repartition at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line"> |  ShuffledRDD[<span class="number">19</span>] at repartition at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line"> +-(<span class="number">3</span>) MapPartitionsRDD[<span class="number">18</span>] at repartition at &lt;console&gt;:<span class="number">25</span> []</span><br><span class="line">    |  ParallelCollectionRDD[<span class="number">17</span>] at parallelize at &lt;console&gt;:<span class="number">23</span> []</span><br><span class="line"></span><br><span class="line">    <span class="number">2</span>.partitionBy----</span><br><span class="line">    scala&gt;  val hashPairs1 = rangePairs1.partitionBy(<span class="keyword">new</span> HashPartitioner(<span class="number">6</span>))</span><br><span class="line">    hashPairs1: org.apache.spark.rdd.RDD[(Int, Char)] = ShuffledRDD[<span class="number">1</span>] at partitionBy at &lt;console&gt;:<span class="number">28</span></span><br><span class="line">    </span><br><span class="line">    scala&gt; hashPairs1.partitions.size</span><br><span class="line">    res0: Int = <span class="number">6</span></span><br><span class="line">    </span><br><span class="line">    scala&gt; println(hashPairs1.toDebugString)</span><br><span class="line">(<span class="number">6</span>) ShuffledRDD[<span class="number">4</span>] at partitionBy at &lt;console&gt;:<span class="number">32</span> []</span><br><span class="line"> +-(<span class="number">3</span>) MapPartitionsRDD[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">30</span> []</span><br><span class="line">    |  ParallelCollectionRDD[<span class="number">2</span>] at parallelize at &lt;console&gt;:<span class="number">26</span> []</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>partition 与 task 关系<br>spark 的每一个stage都包含了一个或多个task，task的数量取决于每个stage中最后一个RDD的partition数量。</p>
</li>
</ul>
<h3 id="Dependency"><a href="#Dependency" class="headerlink" title="Dependency"></a>Dependency</h3><p>RDD 本身的依赖关系由 transformation() 生成的每一个 RDD 本身语义决定，每个RDD的getDependencies()定义RDD之间的数据依赖关系。</p>
<p>RDD 中 partition 依赖关系分为 NarrowDependency（窄依赖） 和 ShuffleDependency（宽依赖）</p>
<p><strong>窄依赖</strong> 指父RDD的每一个分区最多被一个子RDD的分区所用，表现为</p>
<ul>
<li><p>一个父RDD的分区对应于一个子RDD的分区</p>
</li>
<li><p>两个父RDD的分区对应于一个子RDD 的分区。</p>
</li>
</ul>
<p><strong>宽依赖</strong> 指子RDD的每个分区都要依赖于父RDD的所有分区，</p>
<h2 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h2><p>DAGScheduler对Stage的划分是spark任务调度的核心，在上篇 <a href="http://zxdy.github.io/articles/spark-job-logic.html" target="_blank" rel="noopener">spark 源码阅读–job提交与执行过程</a> 提到spark划分stage的总体思想是从最后的finallRDD出发反向递归访问逻辑执行图，每遇到宽依赖就断开，把之前沿途的窄依赖都加入同一个stage。于是对于本文开始的那个RDD数据流转图可以进行如下的划分。</p>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/simple_stage.png" alt="simpe_stage"></p>
<h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><p>讲了这么多理论知识，下面准备结合一个具体的例子来验证一下spark job提交之后的各个过程。</p>
<p>首先构造一个稍复杂的spark job。代码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123;Level, Logger&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"></span><br><span class="line">object complexJob &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span> </span>&#123;</span><br><span class="line">    <span class="comment">//开启debug日志</span></span><br><span class="line">    Logger.getLogger(<span class="string">"org"</span>).setLevel(Level.DEBUG)</span><br><span class="line">    Logger.getLogger(<span class="string">"akka"</span>).setLevel(Level.DEBUG)</span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(<span class="string">"local[3]"</span>, <span class="string">"ComplexJob test"</span>)</span><br><span class="line"></span><br><span class="line">    val data1 = Array[Int](<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    val data2 = Array[Char](<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'g'</span>, <span class="string">'h'</span>, <span class="string">'i'</span>)</span><br><span class="line"></span><br><span class="line">    val dataRdd1 = sc.parallelize(data1, <span class="number">3</span>)</span><br><span class="line">    val dataRdd2 = sc.parallelize(data2, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    val dataRdd3 = dataRdd1.zip(dataRdd2)</span><br><span class="line">    val dataRdd4 = dataRdd3.groupByKey()</span><br><span class="line">    val dataRdd5 = dataRdd4.map(i =&gt; (i._1 + <span class="number">1</span>, i._2))</span><br><span class="line"></span><br><span class="line">    val data3 = Array[(Int, String)]((<span class="number">1</span>, <span class="string">"A"</span>), (<span class="number">2</span>, <span class="string">"B"</span>),</span><br><span class="line">      (<span class="number">3</span>, <span class="string">"C"</span>), (<span class="number">4</span>, <span class="string">"D"</span>))</span><br><span class="line">    val dataRdd6 = sc.parallelize(data3, <span class="number">2</span>)</span><br><span class="line">    val dataRdd7 = dataRdd6.map(x =&gt; (x._1, x._2.charAt(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    val data4 = Array[(Int, Char)]((<span class="number">1</span>, <span class="string">'X'</span>), (<span class="number">2</span>, <span class="string">'Y'</span>))</span><br><span class="line">    val dataRdd8 = sc.parallelize(data4, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    val dataRdd9 = dataRdd7.union(dataRdd8)</span><br><span class="line"></span><br><span class="line">    val result = dataRdd5.join(dataRdd9)</span><br><span class="line">    result.foreach(println)</span><br><span class="line">    println(result.toDebugString)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>执行后打印DebugString如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(4) MapPartitionsRDD[11] at join at complexJob.scala:31 []</span><br><span class="line"> |  MapPartitionsRDD[10] at join at complexJob.scala:31 []</span><br><span class="line"> |  CoGroupedRDD[9] at join at complexJob.scala:31 []</span><br><span class="line"> +-(3) MapPartitionsRDD[4] at map at complexJob.scala:19 []</span><br><span class="line"> |  |  ShuffledRDD[3] at groupByKey at complexJob.scala:18 []</span><br><span class="line"> |  +-(3) ZippedPartitionsRDD2[2] at zip at complexJob.scala:17 []</span><br><span class="line"> |     |  ParallelCollectionRDD[0] at parallelize at complexJob.scala:14 []</span><br><span class="line"> |     |  ParallelCollectionRDD[1] at parallelize at complexJob.scala:15 []</span><br><span class="line"> +-(4) UnionRDD[8] at union at complexJob.scala:29 []</span><br><span class="line">    |  MapPartitionsRDD[6] at map at complexJob.scala:24 []</span><br><span class="line">    |  ParallelCollectionRDD[5] at parallelize at complexJob.scala:23 []</span><br><span class="line">    |  ParallelCollectionRDD[7] at parallelize at complexJob.scala:27 []</span><br></pre></td></tr></table></figure>
<p>从上面的DebugString中，已经可以获取很多信息包括</p>
<ol>
<li>RDD的转换</li>
<li>stage的划分：每一个缺口的同级可以分为一个stage</li>
<li>partition的数量：在RDD前面括号里的数字就代表当前stage最后一个RDD的paritition数量，由此也可知每个stage的task数量</li>
</ol>
<p>DebugString有点像oracle的执行计划，虽然我们已经可以根据这个来划分stage，但是并没有太多的细节信息，所以还是想从代码出发，来详细解释一下stage为什么要这么划分，以及stage提交的流程，顺便也可以与DebugString相互验证。</p>
<h3 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h3><p>根据代码，最终的流程图如下：</p>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/complex_stage.png" alt="compex_stage"></p>
<h3 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h3><p>从整体上看，最后的结果result来自dataRdd5.join(dataRdd9)，所以我们可以首先将整个job拆成A,B两条线，分别对应dataRdd5和dataRdd9的处理流程。</p>
<ul>
<li>A线:</li>
</ul>
<p>A1. 数组data1生成类型为ParallelCollectionRDD的dataRdd1，分区数为3。</p>
<p>A2. 数组data2生成类型为ParallelCollectionRDD的dataRdd2，分区数为3。</p>
<p>A3. 将dataRdd1和dataRdd2通过Transformation(zip)生成新的ZipPartitionsRDD(dataRdd3)。zip是一种变形的map，可以将两个数组根据相同的下标map成一个新的数组结构[(key1,value1),(key2,value2)..(keyN,valueN)]。要注意的是两个RDD的partition数必须相等，否则不能zip。</p>
<p>A4. dataRdd3通过Transformation(groupBykey)生成ShuffledRDD(dataRdd4)。注意此处有shuffle。</p>
<p>A5. dataRdd4通过Transformation(map)将所有的key值加1后生成新的MapPartitionsRDD(dataRdd5)，分区数为3。</p>
<p>dataRdd5生成完毕</p>
<ul>
<li>B线:</li>
</ul>
<p>B1. 数组data3生成类型为ParallelCollectionRDD的dataRdd6，分区数为2。</p>
<p>B2. dataRdd6通过Transformation(map)生成MapPartitionsRDD(dataRdd7)</p>
<p>B3. 数组data4生成类型为ParallelCollectionRDD的dataRdd8，分区数为2。</p>
<p>B4. dataRdd7 与dataRdd8通过Transformation(union)生成新的UnionRDD(dataRdd9)，分区数为4</p>
<p>dataRdd9生成完毕</p>
<p>最后得到result=dataRdd5.join(dataRdd9)，分区数为4。注意此处有shuffle。调用join()的时候并不是一次性生成最后的MapPartitionsRDD，而是首先会进行 cogroup()，得到&lt;K, (Iterable[V1], Iterable[V2])&gt;类型的MapPartitionsRDD，然后对 Iterable[V1] 和 Iterable[V2] 做笛卡尔集，最后生成新的MapPartitionsRDD，所以在join后会产生3个RDD。</p>
<h3 id="stage-划分"><a href="#stage-划分" class="headerlink" title="stage 划分"></a>stage 划分</h3><p>根据DAGScheduler的逻辑，首先从最后的finalRDD(本文为result)开始向前递归访问。</p>
<p>当到达join时，发现有子RDD(result)的每个分区都要依赖于父RDD(dataRdd5和dataRdd9)的所有分区，所以是shuffleDependency,。于是把join之后的所有RDD划分为一个stage，标记为<strong>stage3</strong>。</p>
<blockquote>
<p>注意，假如dataRdd5,dataRdd9的partitioner也是HashPartitioner，且partition数量与result的相同，那么他们之间的依赖就变成了narrowDependency，属于两个(会有多个吗？不,spark只支持两两join)父RDD的分区对应于一个子RDD的分区的情况，这个join()变成了hashjoin()。</p>
</blockquote>
<p>继续往前，先看A线部分，当递归访问到A4步骤(groupBykey)时，判断dataRdd3–&gt;dataRdd4之间的依赖为shuffleDependency，于是将dataRdd4和dataRdd5划分为新的stage，标记为<strong>stage1</strong>。</p>
<p>A线继续往前，到zip又遇到依赖，属于两个父RDD(dataRdd1&amp;dataRdd2)的分区对应于一个子RDD(dataRdd3)的分区的情况，判断为narrowDependency，而dataRdd1和dataRdd2之前已经没有别的RDD存在，于是将dataRdd1，dataRdd2，dataRdd3划分为一个新的stage，标记为<strong>stage0</strong>。</p>
<p>A线已经结束，现在看B线部分。在B线的递归路径中发现有union()，union()是将两个RDD简单合并在一起，并不改变 partition里面的数据。它是一种RangeDependency，属于narrowDependency的一种。继续往前遍历，发现整个B线中都不存在shuffleDependency，所以可以将整个B线划分为新的stage，标记为<strong>stage2</strong>。</p>
<p>到此对整个job的stage划分已经结束，总共分为4个stage。</p>
<h3 id="stage提交"><a href="#stage提交" class="headerlink" title="stage提交"></a>stage提交</h3><p>stage提交的过程，也就是job执行的过程。根据前一篇的spark源代码解读，DAGScheduler首先会调用finalStage = newStage()进行stage划分，这一步已经在上文完成，那么接下来就是DAGScheduler调用submitStage，提交finalStage。</p>
<ol>
<li><p>在本文中例子中的finalStage就是最后的stage3，提交stage3之后，递归查找parentStage，发现stage3依赖于stage1和stage2。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    15/04/27 19:03:28 INFO DAGScheduler: Registering RDD 2 (zip at complexJob.scala:17)</span><br><span class="line">    15/04/27 19:03:28 INFO DAGScheduler: Registering RDD 4 (map at complexJob.scala:19)</span><br><span class="line">    15/04/27 19:03:28 INFO DAGScheduler: Registering RDD 8 (union at complexJob.scala:29)</span><br><span class="line">    15/04/27 19:03:28 INFO DAGScheduler: Got job 0 (foreach at complexJob.scala:32) with 4 output partitions (allowLocal=false)</span><br><span class="line">    15/04/27 19:03:28 INFO DAGScheduler: Final stage: Stage 3(foreach at complexJob.scala:32)</span><br><span class="line">    15/04/27 19:03:28 INFO DAGScheduler: Parents of final stage: List(Stage 1, Stage 2)</span><br><span class="line">    15/04/27 19:03:28 DEBUG DAGScheduler: submitStage(Stage 3)</span><br><span class="line">15/04/27 19:03:28 DEBUG DAGScheduler: missing: List(Stage 1, Stage 2)</span><br></pre></td></tr></table></figure>
</li>
<li><p>提交stage1和stage2，将stage3放入waitingStages。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">15/04/27 19:03:28 DEBUG DAGScheduler: submitStage(Stage 1)</span><br><span class="line">15/04/27 19:03:28 DEBUG DAGScheduler: submitStage(Stage 2)</span><br></pre></td></tr></table></figure>
</li>
<li><p>stage1 发现parentStage：stage0,提交stage0，将stage1放入waitingStages。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">15/04/27 19:03:28 DEBUG DAGScheduler: submitStage(Stage 1)</span><br><span class="line">15/04/27 19:03:28 DEBUG DAGScheduler: missing: List(Stage 0)</span><br><span class="line">15/04/27 19:03:28 DEBUG DAGScheduler: submitStage(Stage 0)</span><br></pre></td></tr></table></figure>
</li>
<li><p>stage0没有找到parentStage,可以立即执行，调用submitMissingTasks，生成类型为ShuffleMapTask的tasks，task的数量与stage0中最后一个Rdd partition数量相等为3。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">15/04/27 19:03:28 DEBUG DAGScheduler: submitStage(Stage 0)</span><br><span class="line">15/04/27 19:03:28 DEBUG DAGScheduler: missing: List()</span><br><span class="line">15/04/27 19:03:28 INFO DAGScheduler: Submitting Stage 0 (ZippedPartitionsRDD2[2] at zip at complexJob.scala:17), which has no missing parents</span><br><span class="line">15/04/27 19:03:28 DEBUG DAGScheduler: submitMissingTasks(Stage 0)</span><br><span class="line">15/04/27 19:03:28 INFO DAGScheduler: Submitting 3 missing tasks from Stage 0 (ZippedPartitionsRDD2[2] at zip at complexJob.scala:17)</span><br><span class="line">15/04/27 19:03:28 DEBUG DAGScheduler: New pending tasks: Set(ShuffleMapTask(0, 2), ShuffleMapTask(0, 1), ShuffleMapTask(0, 0))</span><br><span class="line">15/04/27 19:03:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 3 tasks</span><br></pre></td></tr></table></figure>
</li>
<li><p>stage2没有找到parentStage,可以立即执行，调用submitMissingTasks，生成类型为ShuffleMapTask的tasks，task的数量与stage2中最后一个Rdd partition数量相等为4。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    15/04/27 19:03:28 DEBUG DAGScheduler: submitStage(Stage 2)</span><br><span class="line">    15/04/27 19:03:28 DEBUG DAGScheduler: missing: List()</span><br><span class="line">    15/04/27 19:03:28 DEBUG DAGScheduler: submitMissingTasks(Stage 2)</span><br><span class="line">    15/04/27 19:03:28 INFO DAGScheduler: Submitting 4 missing tasks from Stage 2 (UnionRDD[8] at union at complexJob.scala:29)</span><br><span class="line">    15/04/27 19:03:28 DEBUG DAGScheduler: New pending tasks: Set(ShuffleMapTask(2, 0), ShuffleMapTask(2, 3), ShuffleMapTask(2, 2), ShuffleMapTask(2, 1))</span><br><span class="line">15/04/27 19:03:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks</span><br></pre></td></tr></table></figure>
</li>
<li><p>stage0的task完成后通知dagScheduler，当stage0中所有的task都完成后，将stage0的执行结果注册到mapOutputTrackerMaster给下一个被依赖的stage1使用。然后提交等待中的stage1。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  15/04/27 19:03:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)</span><br><span class="line">15/04/27 19:03:28 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)</span><br><span class="line">15/04/27 19:03:28 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)</span><br><span class="line">15/04/27 19:03:28 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 883 bytes result sent to driver</span><br><span class="line">15/04/27 19:03:28 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 883 bytes result sent to driver</span><br><span class="line">15/04/27 19:03:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 883 bytes result sent to driver</span><br></pre></td></tr></table></figure>
</li>
<li><p>stage2的task全部完成后将结果注册到mapOutputTrackerMaster给下一个被依赖的stage3使用。等待stage1的tasks结束</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">15/04/27 19:03:28 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Running task 2.0 in stage 2.0 (TID 5)</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Running task 3.0 in stage 2.0 (TID 6)</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 884 bytes result sent to driver</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Finished task 2.0 in stage 2.0 (TID 5). 884 bytes result sent to driver</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 884 bytes result sent to driver</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Finished task 3.0 in stage 2.0 (TID 6). 884 bytes result sent to driver</span><br></pre></td></tr></table></figure>
</li>
<li><p>stage0结束之后，stage1调用submitMissingTasks，生成类型为ShuffleMapTask的tasks，task的数量与stage1中最后一个Rdd partition数量相等为3。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">15/04/27 19:03:29 INFO DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[4] at map at complexJob.scala:19), which is now runnable</span><br><span class="line">15/04/27 19:03:29 DEBUG DAGScheduler: submitMissingTasks(Stage 1)</span><br><span class="line">15/04/27 19:03:29 INFO DAGScheduler: Submitting 3 missing tasks from Stage 1 (MapPartitionsRDD[4] at map at complexJob.scala:19)</span><br><span class="line">15/04/27 19:03:29 DEBUG DAGScheduler: New pending tasks: Set(ShuffleMapTask(1, 2), ShuffleMapTask(1, 1), ShuffleMapTask(1, 0))</span><br><span class="line">15/04/27 19:03:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 3 tasks</span><br></pre></td></tr></table></figure>
</li>
<li><p>stage1的tasks全部完成后将结果注册到mapOutputTrackerMaster给下一个被依赖的stage3使用。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">15/04/27 19:03:29 INFO Executor: Running task 1.0 in stage 1.0 (TID 8)</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 7)</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Running task 2.0 in stage 1.0 (TID 9)</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Finished task 1.0 in stage 1.0 (TID 8). 1100 bytes result sent to driver</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Finished task 2.0 in stage 1.0 (TID 9). 1100 bytes result sent to driver</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 7). 1100 bytes result sent to driver</span><br></pre></td></tr></table></figure>
</li>
<li><p>stage3的parentStage已经全部结束任务，提交stage3，生成类型为ResultTask的tasks，task的数量与stage3中最后一个Rdd partition数量相等为4。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">    15/04/27 19:03:29 INFO DAGScheduler: Stage 1 (map at complexJob.scala:19) finished in 0.149 s</span><br><span class="line">15/04/27 19:03:29 INFO DAGScheduler: looking for newly runnable stages</span><br><span class="line">15/04/27 19:03:29 INFO DAGScheduler: running: Set()</span><br><span class="line">15/04/27 19:03:29 INFO DAGScheduler: waiting: Set(Stage 3)</span><br><span class="line">15/04/27 19:03:29 INFO DAGScheduler: failed: Set()</span><br><span class="line">15/04/27 19:03:29 INFO DAGScheduler: Missing parents for Stage 3: List()</span><br><span class="line">15/04/27 19:03:29 INFO DAGScheduler: Submitting Stage 3 (MapPartitionsRDD[11] at join at complexJob.scala:31), which is now runnable</span><br><span class="line">15/04/27 19:03:29 DEBUG DAGScheduler: submitMissingTasks(Stage 3)</span><br><span class="line">15/04/27 19:03:29 INFO DAGScheduler: Submitting 4 missing tasks from Stage 3 (MapPartitionsRDD[11] at join at complexJob.scala:31)</span><br><span class="line">15/04/27 19:03:29 DEBUG DAGScheduler: New pending tasks: Set(ResultTask(3, 2), ResultTask(3, 0), ResultTask(3, 1), ResultTask(3, 3))</span><br><span class="line">15/04/27 19:03:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 4 tasks</span><br></pre></td></tr></table></figure>
</li>
<li><p>stage3的task的结束时判断此task是不是最后一个task，如果是则job结束。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">--output result on each task finish</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Finished task 1.0 in stage 3.0 (TID 11). 886 bytes result sent to driver</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Finished task 2.0 in stage 3.0 (TID 12). 886 bytes result sent to driver</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Finished task 3.0 in stage 3.0 (TID 13). 886 bytes result sent to driver</span><br><span class="line">15/04/27 19:03:29 INFO Executor: Finished task 0.0 in stage 3.0 (TID 10). 886 bytes result sent to driver</span><br><span class="line">15/04/27 19:03:29 INFO DAGScheduler: Job 0 finished: foreach at complexJob.scala:32, took 1.004075 s</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h3><p>本文对于spark job执行过程中关于stage划分，stage提交，task运行的流程已经全部讲解完毕。<br>由于本人才疏学浅以及时间的关系，如有错漏之处，请指出来我会重新修改。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/04/07/spark-job-logic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ario">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2015/04/07/spark-job-logic/" class="post-title-link" itemprop="url">spark 源码阅读--job提交与执行过程</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2015-04-07 00:00:00" itemprop="dateCreated datePublished" datetime="2015-04-07T00:00:00+08:00">2015-04-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-09 14:36:38" itemprop="dateModified" datetime="2019-04-09T14:36:38+08:00">2019-04-09</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>今天终于有时间把spark的源代码看了下，因为之前一直对spark的任务调度流程比较模糊，而网上关于spark的设计实现文档又不太完整，所以这篇文章主要用于梳理这方面的逻辑关系，并不会对代码的细节实现作过多的讨论，我对scala来说，也只能算个新手，许多复杂的语法和实现，无法太深入解读，只能比较宏观地介绍一下其中的逻辑和目的，如果有错漏之处，请多指教。顺便吐槽一下看源代码真是望山跑死马啊，短短一句代码隐藏N多细节。</p>
<p>本文基于<strong>spark 1.3.0</strong></p>
<h2 id="Job-提交"><a href="#Job-提交" class="headerlink" title="Job 提交"></a>Job 提交</h2><p>当我们向spark提交一个application的时候，首先都会调用程序里的val sc = new SparkContext(sparkConf)语句，这一句创建了一个SparkContext实例，确立整个程序作为driver的地位。</p>
<p>我们知道数据在spark中的处理主要分为<a href="http://spark.apache.org/docs/latest/programming-guide.html#transformations" target="_blank" rel="noopener">Transformations</a>和<a href="http://spark.apache.org/docs/latest/programming-guide.html#actions" target="_blank" rel="noopener">action</a>两种类型。</p>
<blockquote>
<ul>
<li>Transformations：属于懒加载，他会先建立一系列的RDD，每个RDD的compute() 定义如何根据上游数据计算当前RDD的结果。每个RDD的getDependencies()定义RDD之间的数据依赖关系。</li>
<li>action：是数据最后的reduce过程，只有当程序运行到action的时候才会真正触发生成一个job，即application中有几个action就会提交几个job。</li>
</ul>
</blockquote>
<p>从SparkContext的实例来出发，原始的RDD经过一连串的transformation操作，转换成为其它类型的RDD，直到遇到action，触发调用SparkContext的runJob方法   </p>
<ol>
<li><p>SparkContext</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">createTaskScheduler</span><br><span class="line">        scheduler = <span class="keyword">new</span> TaskSchedulerImpl</span><br><span class="line">        backend=?</span><br><span class="line">            LocalBackend</span><br><span class="line">            SparkDeploySchedulerBackend</span><br><span class="line">            CoarseGrainedSchedulerBackend</span><br><span class="line">SparkContext.runjob</span><br><span class="line">    dagScheduler.runJob</span><br><span class="line">        submitJob</span><br><span class="line">            eventProcessLoop.post(JobSubmitted())</span><br><span class="line">            onReceive</span><br><span class="line">                dagScheduler.handleJobSubmitted</span><br></pre></td></tr></table></figure>
<p> SparkContext在初始化的时候会新建TaskSchedulerImpl和LocalBackend（本文以local模式为例，如果是standalone或是yarn则分别为SparkDeploySchedulerBackend和CoarseGrainedSchedulerBackend）实例。这两个实例会在之后task执行时用到。</p>
<p> SparkContext.runjob是后面一系列反应的起点，它最终调用的是dagScheduler.handleJobSubmitted方法。</p>
</li>
<li><p>DAGScheduler</p>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dagScheduler.handleJobSubmitted</span><br><span class="line">    finalStage = newStage()</span><br><span class="line">    submitStage(finalStage)</span><br><span class="line">        getMissingParentStages</span><br><span class="line">        submitMissingTasks</span><br><span class="line">            tasks:Seq[]=<span class="keyword">new</span> ShuffleMapTask or ResultTask</span><br><span class="line">            taskScheduler.submitTasks</span><br></pre></td></tr></table></figure>
<p> DAGScheduler在spark中是非常重要的一个组件，spark任务所谓的有向无环图就是通过这个组件生成。</p>
<ul>
<li><p><strong>finalStage = newStage()</strong> 将整个job根据宽依赖和窄依赖进行stage划分（总体的思想是从最后的finallRDD出发反向递归逻辑执行图，每遇到宽依赖就断开，把之前沿途的窄依赖都加入同一个stage）。同时，将每个stage中的最后一个RDD通过mapOutputTracker.registerShuffle注册到MapOutputTrackerMaster，用于指示ShuffleMapTask最后输出数据的位置。</p>
</li>
<li><p><strong>submitStage</strong> 首先调用getMissingParentStages(),确定有没有parentStage，如果有的话，先递归提交parentStage，并将自己加入到 waitingStages 里，直到当前stage没有parentStage，此时stage 可以立即执行，调用submitMissingTasks，根据当前stage的类型（ShuffleMapStage或ResultStage）生成数量跟当前stage最后一个RDD的partition数一样的Tasks（ShuffleMapTasks或ResultTasks）。打包Tasks交给taskScheduler处理。</p>
</li>
</ul>
</li>
<li><p>TaskSchedulerImpl</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TaskSchedulerImpl.submitTasks</span><br><span class="line">     backend.reviveOffers()</span><br></pre></td></tr></table></figure>
<p>TaskSchedulerImpl实现了taskScheduler的接口，这个TaskSchedulerImpl就是之前在第一步产生的TaskSchedulerImpl实例。最后将Tasks交给backend（同样是第一步产生的实例，本文中为了方便使用LocalBackend）处理。</p>
</li>
<li><p>LocalBackend</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">receiveWithLogging</span><br><span class="line">    reviveOffers</span><br><span class="line">        executor.launchTask(..., task.serializedTask)</span><br></pre></td></tr></table></figure>
<p>Backend 接收到 taskSet 后,将序列化后之后的 task 分发到调度器指定的 worker node 上执行</p>
</li>
</ol>
<h2 id="Job-接收"><a href="#Job-接收" class="headerlink" title="Job 接收"></a>Job 接收</h2><ol>
<li><p>Executor</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">executor.launchTask()</span><br><span class="line">    <span class="keyword">new</span> TaskRunner()</span><br><span class="line">        run()</span><br><span class="line">            task = ser.deserialize</span><br><span class="line">            value = task.run()</span><br><span class="line">            serializedResult=?</span><br><span class="line">                IndirectTaskResult</span><br><span class="line">                    write to mem&amp;disk</span><br><span class="line">                serializedDirectResult</span><br><span class="line">            execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)</span><br></pre></td></tr></table></figure>
<p>Executor 收到序列化后的task，首先进行反序列化，然后运行 task 得到执行结果 directResult。序列化directResult后，得到其大小，如果大于 spark.driver.maxResultSize 或者akkaFrameSize - AkkaUtils.reservedSizeBytes，将结果写入内存或磁盘（根据conf配置），由 blockManager 管理，只返回存储位置信息的IndirectTaskResult。否则就将结果serializedDirectResult直接返回给driver。task结束调用execBackend.statusUpdate()。</p>
</li>
<li><p>LocalBackend</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">receiveWithLogging</span><br><span class="line">    scheduler.statusUpdate</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol start="3">
<li><p>TaskSchedulerImpl</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">statusUpdate</span><br><span class="line">    <span class="comment">//IF TaskState.FINISHED</span></span><br><span class="line">    taskResultGetter.enqueueSuccessfulTask()</span><br><span class="line">        serializedTaskResult=blockManager.getRemoteBytes()</span><br><span class="line">        scheduler.handleSuccessfulTask</span><br><span class="line">    taskSetManager.handleSuccessfulTask</span><br><span class="line">         sched.dagScheduler.taskEnded()</span><br></pre></td></tr></table></figure>
<p>通知TaskSchedulerImpl task已经执行完，最后result如果是IndirectTaskResult，则还需调用 blockManager.getRemoteBytes() 去拿到实际的 result。通知dagScheduler Task执行结束。</p>
</li>
<li><p>dagScheduler</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dagScheduler.handleTaskCompletion</span><br><span class="line">    <span class="comment">//IF TASK IS ResultTask</span></span><br><span class="line">        <span class="comment">// If the whole job has finished, remove it</span></span><br><span class="line">        markStageAsFinished</span><br><span class="line">        listenerBus.post(SparkListenerJobEnd)</span><br><span class="line">        job.listener.taskSucceeded</span><br><span class="line">    <span class="comment">//IF TASK IS ShuffleMapTask</span></span><br><span class="line">        <span class="comment">//IF ALL TASKS DONE IN CURRENT STAGE</span></span><br><span class="line">        stage.addOutputLoc(smt.partitionId, status)</span><br><span class="line">        mapOutputTracker.registerMapOutputs()</span><br><span class="line">        submitMissingTasks</span><br></pre></td></tr></table></figure>
<p>dagScheduler 判断当前结束的task类型，假如是ResultTask，继续判断是不是job已经执行完毕。假如是task类型是ShuffleMapTask，判断当前stage的所有task是不是都已经运行完毕，如果是的话将当前stage的执行结果注册到mapOutputTrackerMaster给下一个被依赖的stage使用，并继续提交等待中的stage。</p>
</li>
</ol>
<h2 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h2><p>本文主要介绍了spark job的一个完整生命周期（抱歉没有画图，有空补），下一篇准备结合一个具体的例子来继续讲一下。目前对stage之间的shuffle连接没有做深入研究，shuffle write在task结束后，但是没有发现task开始时的shuffle read，不知是不是被封装在各个RDD的compute里了。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/03/24/soc-doc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ario">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2015/03/24/soc-doc/" class="post-title-link" itemprop="url">综合安全分析</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2015-03-24 00:00:00" itemprop="dateCreated datePublished" datetime="2015-03-24T00:00:00+08:00">2015-03-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-09 14:36:24" itemprop="dateModified" datetime="2019-04-09T14:36:24+08:00">2019-04-09</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>今天翻文档的时候，偶然发现自己去年还写过一篇关于信息安全综合分析的审计方案文档草稿，真是不胜唏嘘。</p>
<p>这篇文档主要从五个方面来审计企业内部的信息安全事件，用于发现和防范黑客的入侵，对企业当前的安全风险有直观的统计</p>
<ol>
<li>网络入侵</li>
<li>账户管理</li>
<li>地理资源</li>
<li>用户操作</li>
</ol>
<p>写这篇文档的时候参考了许多其他soc日志审计平台的设计思想，但是由于时间和一些其他的原因，并没有细化到具体实现，只能算是一份概要设计草稿。</p>
<p>以上，随意感受下就好。</p>
<p><a href="http://7x2wf2.com1.z0.glb.clouddn.com/%E7%BB%BC%E5%90%88%E5%AE%89%E5%85%A8%E5%88%86%E6%9E%90%E6%96%87%E6%A1%A3part2v1.1.pdf" target="_blank" rel="noopener">综合安全分析方案文档</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/03/13/allow-filtering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ario">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2015/03/13/allow-filtering/" class="post-title-link" itemprop="url">ALLOW FILTERING 之谜</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2015-03-13 00:00:00" itemprop="dateCreated datePublished" datetime="2015-03-13T00:00:00+08:00">2015-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-09 14:36:11" itemprop="dateModified" datetime="2019-04-09T14:36:11+08:00">2019-04-09</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>上篇在讲到cassandra查询语句之坑的时候，有提到一个叫allow filtering的东东，但是令人费解的是，这货有时候出现有时候又不用出现。那到底这是怎么用的呢，让我们来用一个例子说明。首先还是沿用上篇的table2：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> table2 (</span><br><span class="line">  key_part_one <span class="built_in">text</span>,</span><br><span class="line">  key_part_two <span class="built_in">int</span>,</span><br><span class="line">  <span class="keyword">data</span> <span class="built_in">text</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span>(key_part_one, key_part_two)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>此时我们有了一个主键（key_part_one, key_part_two），一个分区键:key_part_one，一个clustering key：key_part_two。</p>
<p>如果你执行下面一个查询：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> table2;</span><br></pre></td></tr></table></figure>
<p>Cassandra会返回所有table2表中的数据（假如table2表的数据量非常大，这种查询会导致rpc timeout，可以在Cassandra配置文件中适当加大timeout的时间来解决这一问题，但是个人感觉治标不治本。目前发现用spark+Cassandra的方式可以比较好的处理大数据量的存储以及分析，有时间我会写一篇spark+Cassandra的集成攻略）。</p>
<p>按照关系型数据库的习惯，你可能还会做如下的查询：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> table2 <span class="keyword">WHERE</span> key_part_two = <span class="number">1111</span>;</span><br></pre></td></tr></table></figure>
<p>然后cassandra就提示你要不要allow filtering一下：</p>
<blockquote>
<p>Bad Request: Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING.</p>
</blockquote>
<p>这是因为Cassandra发现它可能不能很效率地执行这个查询，所以做出了警告：”Be careful. Executing this query as such might not be a good idea as it can use a lot of your computing resources”.</p>
<p>跟所有的nosql数据库一样，cassandra存储数据的方式是key-value，这决定了它查询数据的模式是根据key一层层往下找数据。cassandra执行这个查询的唯一方式是取出所有的行，然后过滤得到key_part_two = 1111的数据。</p>
<p>类似于oracle的统计信息直方图倾斜，key_part_two的值分布可能会有两种极端的情况，一种是绝大多数的key_part_two都等于1111。另一种是绝大多数的key_part_two都不等于1111。第一种情况比较适用allow filtering，因为取出的所有行基本已经是最后的需要的结果集，效率还算可以。但是第二种情况因为取出的所有行要过滤掉绝大多数的数据才是最终结果集，所以用allow filtering的性能会非常低下，经常对这个字段查询的话还是对它增加二级索引会更加好一点。</p>
<p>cassandra提示allow filtering的本质是它认为当前的查询可能会有很大的性能问题，让你决定是不是强制执行，这就是他的意义。因此当你的CQL语句被cassandra拒绝执行的时候，你需要考虑你的数据模型以及你的目的去做出最优的选择，比如说改变数据模型，增加二级索引或者换一张表查询，而不是立刻就不假思索的根据提示加上allow filtering。</p>
<p>参考文章：<br><a href="http://blog.websudos.com/2014/08/a-series-on-cassandra-part-2-indexes-and-keys/" target="_blank" rel="noopener">A series on Cassandra – Part 2: Indexes and keys</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/03/12/cassandra-key/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ario">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2015/03/12/cassandra-key/" class="post-title-link" itemprop="url">Cassandra partition key, composite key 和 clustering key 的区别</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2015-03-12 00:00:00" itemprop="dateCreated datePublished" datetime="2015-03-12T00:00:00+08:00">2015-03-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-09 14:35:59" itemprop="dateModified" datetime="2019-04-09T14:35:59+08:00">2019-04-09</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在Cassandra中，primary key是一个非常重要的概念。关系型数据库中表可以没有primary key（主键），但是Cassandra中建表时必须指定primary key，它不仅决定了表的结构，而且还对数据查询方式的差异有巨大影响。partition key, composite key 和 clustering key共同组成了Cassandra的primary key。</p>
<p>为了说明它们的不同，我们先来看三张表。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> table1 (</span><br><span class="line">pri_key <span class="built_in">text</span> PRIMARY <span class="keyword">KEY</span>,</span><br><span class="line"><span class="keyword">data</span> <span class="built_in">text</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> table2 (</span><br><span class="line">  key_part_one <span class="built_in">text</span>,</span><br><span class="line">  key_part_two <span class="built_in">int</span>,</span><br><span class="line">  <span class="keyword">data</span> <span class="built_in">text</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span>(key_part_one, key_part_two)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> table3 (</span><br><span class="line">  key_part_one <span class="built_in">text</span>,</span><br><span class="line">  key_part_two <span class="built_in">int</span>,</span><br><span class="line">  key_clust_one <span class="built_in">text</span>,</span><br><span class="line">  key_clust_two <span class="built_in">int</span>,</span><br><span class="line">  key_clust_three <span class="keyword">uuid</span>,</span><br><span class="line">  <span class="keyword">data</span> <span class="built_in">text</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span>((key_part_one,key_part_two), key_clust_one, key_clust_two, key_clust_three)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>##格式区别</p>
<p><strong>table1</strong>是初级版，primary key最简单的定义方式就是这样。此时pri_key就是partition key，clustering key。</p>
<p><strong>table2</strong>是table1的升级版，格式为（key1，key2，key3，…）。key_part_one, key_part_two共同组成了primary key，这种方式即所谓的composite key（组合键）。其中key_part_one是partition key，key_part_two共同组成了primary是clustering key。</p>
<p><strong>table2</strong>是高级版。这个primary key的组成最为复杂，格式为（（key1，key2），key3，key4，…）。其中key_part_one,key_part_two 共同组成了partition key。而外边的key_clust_one，key_clust_two, key_clust_three都是clustering key。</p>
<p>##实际的用途</p>
<p>Partition Key ：决定了数据在Cassandra各个节点的是如何分区的。</p>
<p>Clustering Key ： 用于在各个分区内的排序。</p>
<p>Primary Key ： 主键，决定数据行的唯一性</p>
<p>Composite Key ：只是一个多字段组合的概念</p>
<p>跟关系型数据库一样，分区都是为了解决大数据量查询的效率问题，所不同的是Cassandra的分区分布在各个节点上。注意同一个分区的数据是在同一个物理节点上的，这就造成一个问题，假如分区内的数据量过大的话，会造成Cassandra读取负载的不均衡，可以用类似于table3的建表方式，多个字段共同组成一个partition key减小单个分区的大小，使各个分区能够更均匀地分布在节点上，从而实现负载均衡。</p>
<h2 id="cassandra-查询之坑"><a href="#cassandra-查询之坑" class="headerlink" title="cassandra 查询之坑"></a>cassandra 查询之坑</h2><p>在实际的使用过程中,cassandra的数据查询有很多不同于关系型数据库的地方，如果你总是用关系型数据库的思维去考虑cassandra的问题的话，往往会掉进坑里。cassandra的CQL写法并没有像你想象中的随心所欲，因为究其本质，它的数据集是以key-value的形式存放，所以在查询时会有很多限制。</p>
<p>让我们来看下上面三个表的查询语法会有哪些坑。</p>
<p><strong>table1</strong> PRIMARY KEY(pri_key)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table1 <span class="keyword">where</span> pri_key=<span class="string">'111'</span>; <span class="comment">--good</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table1 <span class="keyword">where</span> <span class="keyword">data</span>=<span class="string">'111'</span>; <span class="comment">--error</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table1 <span class="keyword">where</span> pri_key=<span class="string">'111'</span> <span class="keyword">and</span> <span class="keyword">data</span>=<span class="string">'111'</span>; <span class="comment">--error</span></span><br></pre></td></tr></table></figure>
<p><strong>table2</strong> PRIMARY KEY(key_part_one, key_part_two)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table2 <span class="keyword">where</span> key_part_one=<span class="string">'111'</span>; <span class="comment">--good</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table2 <span class="keyword">where</span> key_part_two=<span class="number">111</span>; <span class="comment">--need allow filtering</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table2 <span class="keyword">where</span> key_part_one=<span class="string">'111'</span> <span class="keyword">and</span> key_part_two=<span class="number">111</span>; <span class="comment">--good</span></span><br></pre></td></tr></table></figure>
<p><strong>table3</strong> PRIMARY KEY((key_part_one,key_part_two), key_clust_one, key_clust_two, key_clust_three)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table3 <span class="keyword">where</span> key_part_one=<span class="string">'111'</span>; <span class="comment">--error</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table3 <span class="keyword">where</span> key_part_two=<span class="number">111</span>; <span class="comment">--error</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table3 <span class="keyword">where</span> key_part_one=<span class="string">'111'</span> <span class="keyword">and</span> key_part_two=<span class="number">111</span>; <span class="comment">--good</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table3 <span class="keyword">where</span> key_part_one=<span class="string">'111'</span> <span class="keyword">and</span> key_part_two=<span class="number">111</span> <span class="keyword">and</span> key_clust_one=<span class="string">'111'</span>; <span class="comment">--good</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table3 <span class="keyword">where</span> key_part_one=<span class="string">'111'</span> <span class="keyword">and</span> key_part_two=<span class="number">111</span> <span class="keyword">and</span> key_clust_two=<span class="string">'111'</span>; <span class="comment">--error</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> table3 <span class="keyword">where</span> key_clust_one=<span class="string">'111'</span>; <span class="comment">--need allow filtering</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>总结：</p>
<ol>
<li>cassandra的查询必须在主键列上，或者查询的字段有二级索引。</li>
<li>对于（A，B）形式的主键，假如查询条件不带分区键A，则查询语句需要开启allow filtering。</li>
<li>对于（（A，B）,C,D）形式的主键，可以认为是第2点的变种。A，B必须同时出现在查询条件中,且C,D不可以跳跃，像where A and B and D的查询是非法的。</li>
<li>以上查询不考虑范围查询的情况。</li>
</ol>
</blockquote>
<p>所以因为第三点的关系，parition key字段过多会对以后的查询造成很大困扰，在建表的时候首先一定要考虑好数据模型，以免后期掉坑。此外假如与spark集成的话，可以在一定程度上规避掉上面非法查询的问题，通过sparksql可以近似实现关系型数据库sql的查询，而不用考虑查询中一定要带上所有partition key字段。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/03/10/oracle-tunning-cpu-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ario">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2015/03/10/oracle-tunning-cpu-2/" class="post-title-link" itemprop="url">oracle性能优化-CPU篇(下)</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2015-03-10 00:00:00" itemprop="dateCreated datePublished" datetime="2015-03-10T00:00:00+08:00">2015-03-10</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-09 14:35:44" itemprop="dateModified" datetime="2019-04-09T14:35:44+08:00">2019-04-09</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>接上篇 <a href="http://zxdy.github.io/articles/oracle-tunning-cpu-1.html" target="_blank" rel="noopener">oracle性能优化-CPU篇(上)</a></p>
<p>如何有效利用好CPU，主要在于如何写好SQL语句， 并优化数据库内部处理。</p>
<h2 id="1-sql语句重用"><a href="#1-sql语句重用" class="headerlink" title="1. sql语句重用"></a>1. sql语句重用</h2><h3 id="1-1-硬解析与软解析"><a href="#1-1-硬解析与软解析" class="headerlink" title="1.1 硬解析与软解析"></a>1.1 硬解析与软解析</h3><p>相信用过oracle的人，特别是用oracle做后端数据库开发的程序猿都听说过硬解析和软解析。</p>
<p>当一个sql语句提交后，oracle会首先检查一下共享缓冲池（shared pool）里有没有与之<strong>完全相同</strong>的语句，如果有的话只须执行软分析即可，否则就得进行硬分析。</p>
<p>硬解析之所以坑爹的原因是，它需要经解析,制定执行路径,优化访问计划等许多的步骤。不但耗费大量的cpu，更重要的是会占据重要的们闩（latch）资源，严重的影响系统的规模的扩大（即限制了系统的并发行），而且引起的问题不能通过增加内存条和cpu的数量来解决。</p>
<p>看一眼AWR报表，检查是不是有很多硬解析。下图的硬解析数和 time model statistics的hard parse elapsed time对应，可知该系统是否 是 解析敏感</p>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/hard_prase.png" alt="hard parse"></p>
<p>有时候当我们在oracle上对某些sql进行测试时，会发现第一次执行的sql比较慢，再次执行往往都比较快，这不仅仅是因为所需要的数据块已经读取到buffer cache中了，还因为再次执行的sql沿用的是上次执行的执行计划，并没有重新做解析。</p>
<p>因此假如我们需要测试相同sql不同的执行计划时，最好刷新一下当前session的shared pool。注意最好不要全局刷新，特别是在生产环境，这样会导致所有的sql都进行重新解析，可能会严重影响性能。</p>
<h3 id="1-2-绑定变量"><a href="#1-2-绑定变量" class="headerlink" title="1.2 绑定变量"></a>1.2 绑定变量</h3><p>绑定变量的实质就是用于替代sql语句中的常量的替代变量，它能够使得每次提交的sql语句都完全一样。还记得前面说的”oracle会首先检查一下共享缓冲池（shared pool）里有没有与之<strong>完全相同</strong>的语句”没有？<br>类似这样：</p>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/bind_var.png" alt="bind var"></p>
<p>一般来说，在实际的开发场景，只要使用比较成熟的数据持久层框架例如mybatis等，基本都可以避免这种因为没有用绑定变量而产生的性能问题。</p>
<h3 id="1-3-索引优化"><a href="#1-3-索引优化" class="headerlink" title="1.3 索引优化"></a>1.3 索引优化</h3><p>索引优化这是大坑。。有空新开一篇再写吧</p>
<h2 id="2-表连接"><a href="#2-表连接" class="headerlink" title="2. 表连接"></a>2. 表连接</h2><p>在CBO（hash join只有在CBO才可能被使用到）模式下，优化器计算代价时，首先会考虑hash join。</p>
<ul>
<li>hash join的主要资源消耗在于CPU和内存（在内存中创建临时的hash表，并hash计算， Mem访问速度是Disk的万倍以上。）</li>
<li>Nested Loop资源消耗在磁盘IO和CPU。</li>
<li>sort merge的资源消耗主要在于磁盘IO（扫描表或索引）</li>
</ul>
<p>相对来说，比较常见的还是hash join以及nested loop。</p>
<p>表连接科普参见 <a href="http://blog.csdn.net/tianlesoftware/article/details/5826546" target="_blank" rel="noopener">多表连接的三种方式</a></p>
<h3 id="2-2-hash-join"><a href="#2-2-hash-join" class="headerlink" title="2.2 hash join"></a>2.2 hash join</h3><p> Hash join的工作方式是将一个表（通常是小一点的那个表）做hash运算，将列数据存储到hash列表中，从另一个表中抽取记录，做hash运算，到hash 列表中找到相应的值，做匹配。</p>
<h3 id="2-2-nested-loop"><a href="#2-2-nested-loop" class="headerlink" title="2.2 nested loop"></a>2.2 nested loop</h3><p>Nested loops 工作方式是从一张表中读取数据，访问另一张表（通常是索引）来做匹配，nested loops适用的场合是当一个关联表比较小的时候，效率会更高。</p>
<h3 id="2-2-sort-merge"><a href="#2-2-sort-merge" class="headerlink" title="2.2 sort merge"></a>2.2 sort merge</h3><p>Merge Join 是先将关联表的关联列各自做排序，然后从各自的排序表中抽取数据，到另一个排序表中做匹配，因为merge join需要做更多的排序，所以消耗的资源更多。 通常来讲，能够使用merge join的地方，hash join都可以发挥更好的性能。</p>
<blockquote>
<p>总的来说，小表和大表连接用nested loop，大表和大表连接用hash join，关联列已经排序好的表连接推荐用sort merge（用的不多就是了）。<br>但凡事总有例外，具体的执行效果还是要根据具体情况看，有时候CBO推荐的执行计划未必是最好的， 此时你可以尝试使用hint或者修改sql语句来改变表连接。</p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/03/04/oracle-tunning-cpu-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ario">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2015/03/04/oracle-tunning-cpu-1/" class="post-title-link" itemprop="url">oracle性能优化-CPU篇(上)</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2015-03-04 00:00:00" itemprop="dateCreated datePublished" datetime="2015-03-04T00:00:00+08:00">2015-03-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-09 14:35:29" itemprop="dateModified" datetime="2019-04-09T14:35:29+08:00">2019-04-09</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在任何一样计算机软件产品中，当我们需要考虑其性能的时候，往往都会从CPU，IO，Network这三个方面考虑。CPU代表着处理问题的能力，IO代表着存储的吞吐能力，Network代表着数据传输的能力。oracle当然也不例外。</p>
<p>下图反映的是一个应用程序总体的响应时间的分布情况。用户在前端发出数据的请求之后，经过网络层，到达数据库服务器。数据库服务器接收请求，然后对SQL进行语法语义分析，然后生成执行计划，接着执行sql，取得数据最后再次经过网络层返回到前端展现给用户。</p>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/bottle-neck-of-oracle.png" alt="bottle-neck-of-oracle"></p>
<p>很显然 <strong>oracle的处理时间=cpu处理时间+[资源等待时间+Disk IO 等待时间]</strong>。</p>
<p>根据以上公式，我们可以发现只要能有效地利用cpu资源和降低等待时间就可以提高数据库的性能，使它处理得更快。</p>
<p>这篇文章主要关注oracle sql在cpu上的性能优化。个人感觉相比等待上的优化简单一些，等待很多时候涉及到并发，资源争用，数据块等知识，优化也更加复杂不好下手。</p>
<p><strong>那么问题来了：</strong></p>
<h2 id="1-首先，怎么查看CPU信息"><a href="#1-首先，怎么查看CPU信息" class="headerlink" title="1. 首先，怎么查看CPU信息"></a>1. 首先，怎么查看CPU信息</h2><p>CPU的多少在很大程度上（质量也是很重要滴）决定了数据库性能的好坏。越多的CPU，可以并发的数目就越多。</p>
<ul>
<li><p>用系统命令查看。这里我默认认为oracle是安装在linux服务器上，当然装在windows上的不是没有，只是略奇葩了。</p>
<p>  refer to  <a href="http://zxdy.github.io/articles/linux-info-check.html" target="_blank" rel="noopener">查看cpu信息</a></p>
</li>
<li><p>sql查询NUM_CPUS字段。v$osstat这张表包含了很多有用的信息<br>附上<a href="http://docs.oracle.com/cd/E11882_01/server.112/e40402/dynviews_2085.htm#REFRN30321" target="_blank" rel="noopener">官方文档</a>解释</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> v$osstat;</span><br></pre></td></tr></table></figure>
<ul>
<li>awr报告。在awr中会有cpu相关的各种报告，包括硬件信息以及更重要的性能信息。关于cpu的性能分析我会在后面展开。下面这张图展现了当前实例使用的cpu硬件信息。</li>
</ul>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/awr_cpu.png" alt="awr cpu"></p>
<blockquote>
<p>CPUs: 逻辑cpu数<br>Cores: cpu核数<br>Sockets: 物理cpu数</p>
</blockquote>
<h2 id="2-怎么看CPU的性能"><a href="#2-怎么看CPU的性能" class="headerlink" title="2. 怎么看CPU的性能"></a>2. 怎么看CPU的性能</h2><p>上面的CPU硬件信息只是帮助我们有个初步的概念，如果说你的数据库性能很差，CPU又很烂，很烂还没几个，那你真的该先换CPU了。。</p>
<p>换完CPU，我们先来了解下下面这两个概念：</p>
<ul>
<li>host cpu</li>
<li>instance cpu</li>
</ul>
<p>在AWR报告中会有这样两个不同的分类，像是这样</p>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/awr_cpu.png" alt="host cpu"></p>
<p>还有这样</p>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/instance_cpu.png" alt="instance cpu"></p>
<p>它们其实是分别代表了服务器cpu的负载和oracle实例的负载情况。</p>
<p>对于host cpu：</p>
<ul>
<li>“Load Average” begin/end值代表CPU的大致运行队列大小。上图中快照开始<br>到结束，平均 CPU负载减少了。</li>
<li>%User+%System=&gt; 总的CPU使用率，在这里是5.7%。</li>
<li>Busy Time=Elapsed Time * NUM_CPUS * CPU utilization</li>
</ul>
<p>对于instance cpu:</p>
<ul>
<li>%Total CPU,该实例所使用的CPU占总CPU的比例 -&gt; % of total CPU for<br>Instance</li>
<li>%Busy CPU，该实例所使用的Cpu占总的被使用CPU的比例 -&gt; % of busy CPU for Instance。例如共4个逻辑CPU，其中3个被完全使用， 3个中的1 个完全被该实例使用，则%Total CPU= 1/4 =25%，而%Busy CPU= 1/3= 33%</li>
<li>当CPU高时一般看%Busy CPU可以确定CPU到底是否是本实例消耗的，还是<br>主机上其他程序。</li>
</ul>
<p>身为一个开发我还要时不时看awr报告也是蛮拼的。一般来讲，awr更多的是展现数据库整体上的性能分析，你可能在以上的host cpu以及instance cpu上发现cpu的负载很高，但这又有什么用呢？是不是觉得没法继续了呢？当然不是，awr还提供了更多的详细的报告帮助我们定位哪些sql的cpu占用比较厉害：</p>
<ol>
<li>AWR SQL ordered by Elapsed Time：</li>
</ol>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/sql_order_by_elapsed_time.png" alt="AWR SQL ordered by Elapsed Time"></p>
<blockquote>
<p>%CPU - CPU Time as a percentage of Elapsed Time -&gt; 这个语句耗费的DB TIME里CPU TIME占多少比例 -&gt; 这个语句是否是CPU敏感的</p>
</blockquote>
<p>2.AWR SQL ordered by CPU Time：</p>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/sql_order_by_cpu_time.png" alt="AWR SQL order by cpu time"></p>
<blockquote>
<p>第一列CPU TIME统计这个sql总共在快照时间内总共花费的cpu时间，在这个值比较高的情况下，如果相应的%CPU值也很高， 说明这个sql在cpu上的负载很高，需要考虑优化。</p>
</blockquote>
<p><strong>如果发现有比较突出的sql，很可能就是瓶颈所在，可以继续跑个@?/rdbms/admin/awrsqrpt.sql看看</strong></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/02/09/oracle-tunning-sqlplan/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Ario">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2015/02/09/oracle-tunning-sqlplan/" class="post-title-link" itemprop="url">oracle性能优化-执行计划篇</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2015-02-09 00:00:00" itemprop="dateCreated datePublished" datetime="2015-02-09T00:00:00+08:00">2015-02-09</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-04-09 14:35:16" itemprop="dateModified" datetime="2019-04-09T14:35:16+08:00">2019-04-09</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="执行计划？"><a href="#执行计划？" class="headerlink" title="执行计划？"></a>执行计划？</h1><p>执行计划是sql在数据库中最终的执行路径，包括索引的扫描，数据的读取，过滤，连接，排序等等一系列过程。就像平常在生活中你为了完成一个任务，需要经过很多步骤，如果没有很好的统筹规划，任务时间会不断延长。所以sql的效率跟它的执行计划息息相关，同一个sql可能会有很多不一样的执行计划，基于CBO的oracle会在其中挑选出它认为最快的执行计划进行执行。下面这个流程图展现了优化器选择执行计划过程：</p>
<p> <img src="http://7x2wf2.com1.z0.glb.clouddn.com/sql%20plan.png" alt="优化器选择执行计划过程"></p>
<p> 从上面的流程图可以知道一个高效良好的执行计划需要考虑的因素有</p>
<ol>
<li>绑定变量。//还记得CPU优化篇的sql重用吗</li>
<li>统计信息。oracle默认会在每天的晚上以及周末触发自动收集数据改变量在10%以上的表的统计信息。如果统计信息过于陈旧，DBA或者开发也可以手动提交收集统计信息任务。</li>
<li>hint。hint会强制指定执行计划的路径，比如select /*+ use_hash*/ ,不管优化器怎么认为表连接应该使用nested loop更加效率，都会强制使用hash连接</li>
<li>sql profile。<a href="https://docs.oracle.com/database/121/TGSQL/tgsql_profiles.htm#TGSQL599" target="_blank" rel="noopener">定义</a></li>
<li>sql plan。<a href="https://docs.oracle.com/database/121/TGSQL/tgsql_spm.htm#TGSQL617" target="_blank" rel="noopener">sql profile vs plan baseline</a></li>
</ol>
<h1 id="怎么查看执行计划"><a href="#怎么查看执行计划" class="headerlink" title="怎么查看执行计划"></a>怎么查看执行计划</h1><h2 id="1-Explain-Plan-For-SQL"><a href="#1-Explain-Plan-For-SQL" class="headerlink" title="1. Explain Plan For SQL"></a>1. Explain Plan For SQL</h2><ul>
<li>不实际执行SQL诧句，生成的计划未必是真实执行的计划 </li>
<li>必须要有plan_table</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">EXPLAIN</span> PLAN <span class="keyword">FOR</span></span><br><span class="line">  <span class="keyword">SELECT</span> object_id <span class="keyword">FROM</span> dba_objects;</span><br><span class="line"><span class="keyword">SELECT</span> PLAN_TABLE_OUTPUT <span class="keyword">FROM</span> <span class="keyword">TABLE</span>(DBMS_XPLAN.DISPLAY());</span><br></pre></td></tr></table></figure>
<h2 id="2-SQLPLUS-AUTOTRACE"><a href="#2-SQLPLUS-AUTOTRACE" class="headerlink" title="2. SQLPLUS AUTOTRACE"></a>2. SQLPLUS AUTOTRACE</h2><ul>
<li>除set autotrace traceonly explain外均实际执行SQL，但仍未必是真实计划</li>
<li>必须要有plan_table</li>
</ul>
<h2 id="3-其他第三方工具"><a href="#3-其他第三方工具" class="headerlink" title="3. 其他第三方工具"></a>3. 其他第三方工具</h2><ul>
<li>sqldeveloper</li>
<li>toad（收费，比sqldeveloper强大，细节丰富）</li>
</ul>
<h2 id="4-最靠谱的方法"><a href="#4-最靠谱的方法" class="headerlink" title="4. 最靠谱的方法"></a>4. 最靠谱的方法</h2><p>为什么上文说未必是真实的计划呢？这里的指的是实际生产环境使用的执行计划和你手动用上文的方法查看某个sql的执行计划是有可能有差异的。所以最靠谱的办法是在生产环境找到已经执行过的sql或者是正在执行的sql的sql id，然后根据这个id去查看他的执行计划</p>
<h3 id="查询历史sql和sql-id"><a href="#查询历史sql和sql-id" class="headerlink" title="查询历史sql和sql id"></a>查询历史sql和sql id</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> sql_id,sql_text <span class="keyword">from</span> v$<span class="keyword">SQL</span> <span class="keyword">Where</span></span><br><span class="line">sql_text <span class="keyword">not</span> <span class="keyword">like</span> <span class="string">'%like%'</span></span><br><span class="line"><span class="keyword">and</span> sql_text <span class="keyword">like</span> <span class="string">'%$SQL%'</span>;</span><br><span class="line"><span class="comment">--或者</span></span><br><span class="line"><span class="keyword">select</span> b.SQL_TEXT,b.FIRST_LOAD_TIME,b.SQL_FULLTEXT</span><br><span class="line"><span class="keyword">from</span> v$sqlarea b</span><br><span class="line"><span class="keyword">where</span> b.FIRST_LOAD_TIME <span class="keyword">between</span> <span class="string">'2014-10-15/09:24:47'</span> <span class="keyword">and</span></span><br><span class="line"><span class="string">'2014-10-15/09:24:47'</span> <span class="keyword">order</span> <span class="keyword">by</span> b.FIRST_LOAD_TIME</span><br></pre></td></tr></table></figure>
<h3 id="查询正在执行的sql和sql-id"><a href="#查询正在执行的sql和sql-id" class="headerlink" title="查询正在执行的sql和sql id"></a>查询正在执行的sql和sql id</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.username, a.sid,b.SQL_TEXT, b.SQL_FULLTEXT,b.sql_id</span><br><span class="line"><span class="keyword">from</span> v$<span class="keyword">session</span> a, v$sqlarea b </span><br><span class="line"><span class="keyword">where</span> a.sql_address = b.address ;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>note:</p>
</blockquote>
<blockquote>
<p>V\$SQL, V\$SQLAREA, V\$SQLTEXT三个视图的区别:</p>
</blockquote>
<blockquote>
<p>V\$SQL：CHILD CURSOR DETAILS FOR V\$SQLAREA</p>
</blockquote>
<blockquote>
<p>V\$SQLAREA：SHARED POOL DETAILS FOR STATEMENTS/ANONYMOUS BLOCKS</p>
</blockquote>
<blockquote>
<p>V\$SQLTEXT：SQL TEXT OF STATEMENTS IN THE SHARED POOL</p>
</blockquote>
<blockquote>
<p>V\$SQL的每一行表示的是每一个SQL语句的一个版本，而V\$SQLAREA存放的是相同语句不同版本一个GROUP BY汇总。</p>
</blockquote>
<blockquote>
<p>V\$SQL及V\$SQLAREA存放着统计信息在调优时使用居多，但其SQL是不全的，如果想获得完整的SQL需使用V\$SQLTEXT。</p>
</blockquote>
<h3 id="精确查询详细执行计划"><a href="#精确查询详细执行计划" class="headerlink" title="精确查询详细执行计划"></a>精确查询详细执行计划</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">session</span> <span class="keyword">set</span> STATISTICS_LEVEL = <span class="keyword">ALL</span>; <span class="comment">--不设置无法获得A-ROWS等信息,A-Rows 是实际执行时返回的行数</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">table</span>(dbms_xplan.display_cursor(<span class="string">'sql_id'</span>,<span class="literal">null</span>,<span class="string">'ADVANCED ALLSTATS LAST PEEKED_BINDS'</span>));</span><br></pre></td></tr></table></figure>
<h1 id="执行计划的执行顺序"><a href="#执行计划的执行顺序" class="headerlink" title="执行计划的执行顺序"></a>执行计划的执行顺序</h1><p>toad看这个最方便，可以直接显示执行顺序。如图中的12345678就是：</p>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/sqlplan_toad.png" alt="sql plan by toad"></p>
<p>但是如果没有toad的话，通常显示的结果会是这样，要注意的是这里的id并不是顺序：</p>
<p><img src="http://7x2wf2.com1.z0.glb.clouddn.com/sqlplan_sqldeveloper.png" alt="sql plan"></p>
<blockquote>
<p>note:</p>
<ul>
<li>Id 分配给执行计划中每一个步骤的一个数字 每个步骤（执行计划中的行，戒树中的节点）代表行源 (row source)。</li>
<li>Operation 该步骤实施的内部操作名 id=0的operation一般是 SELECT/INSERT/UPDATE/DELETE Statement</li>
<li>Name 该步骤操作的表戒者索引名</li>
<li>Rows CBO基亍统计信息估计该操作将返回的行数</li>
<li>Bytes CBO基亍统计信息估计该操作将返回的字节数</li>
</ul>
</blockquote>
<p>如果你学过二叉树的后序遍历的话，其实也很简单。首先我们先将这个图根据缩进转化成一个树。然后对这个树进行一下后续遍历，这个遍历的顺序（4 -&gt; 6 -&gt; 5-&gt; 3 -&gt; 7 -&gt; 2 -&gt; 8 -&gt; 1 -&gt; 0）就是执行顺序。</p>
<p> <img src="http://7x2wf2.com1.z0.glb.clouddn.com/tree.png" alt="执行计划树"></p>
<h1 id="如何通过执行计划优化sql"><a href="#如何通过执行计划优化sql" class="headerlink" title="如何通过执行计划优化sql"></a>如何通过执行计划优化sql</h1><ul>
<li>在计划中，驱动表具有最强的过滤性。</li>
<li>每个步骤的联接顺序都可保证返回给下一步的行数最少（即，联接顺序应使系统转到尚未使用的最强过滤器）。</li>
<li>就返回的行数而言，相应的联接方法是适合的。例如，返回的行很多时，使用索引的嵌套循环联接可能不是最佳方法。</li>
<li>高效地使用视图。查看 SELECT 列表，确定访问的视图是否必需。</li>
<li>是否存在预料之外的笛卡尔积（即使对于小表，也是如此）。</li>
<li>高效地访问每个表：考虑 SQL 语句中的谓词和表的行数。查找可疑活动，例如对行数很多的表执行全表扫描（在 WHERE 子句中有谓词）。而对于小表，或根据返回的行数利用更好的联接方法（例如 hash_join）时，全表扫描也许更有效。</li>
</ul>
<p>参考：</p>
<ol>
<li><a href="http://t.askmaclean.com/thread-3237-1-1.html" target="_blank" rel="noopener">Maclean Liu的Oracle性能优化讲座 真正读懂Oracle SQL执行计划</a></li>
<li><a href="http://blog.itpub.net/22664653/viewspace-701711/" target="_blank" rel="noopener">如何清除某条SQL在库缓存中的执行计划：dbms_shared_pool.purge </a></li>
<li><a href="http://files.cnblogs.com/files/kerrycode/ORACLE_11g_ARCHITECTURE.pdf" target="_blank" rel="noopener">oracle 11g 架构图</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Ario</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">21</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ario</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.1.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/affix.js?v=7.1.0"></script>

  <script src="/js/schemes/pisces.js?v=7.1.0"></script>




  

  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  



  




  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
